{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Classification with Transformers"
      ],
      "metadata": {
        "id": "pp2VQea4WosZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0. How does tiktoken library works?"
      ],
      "metadata": {
        "id": "owxX5qUzYELB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[tiktoken library for part II](#scrollTo=iU7qA3wgxKmS&line=1&uniqifier=1)"
      ],
      "metadata": {
        "id": "qEjjUusGYPrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Download and Preprocess the IMDB Dataset"
      ],
      "metadata": {
        "id": "Yg3GTi2iWjgK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWTrE9VO1B8e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "# Download the dataset\n",
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "dataset_path = 'aclImdb_v1.tar.gz'\n",
        "if not os.path.exists(dataset_path):\n",
        "    urllib.request.urlretrieve(url, dataset_path)\n",
        "\n",
        "# Extract the dataset\n",
        "if not os.path.exists('aclImdb'):\n",
        "    with tarfile.open(dataset_path, 'r:gz') as tar:\n",
        "        tar.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def load_data_from_directory(directory):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for label_type in ['neg', 'pos']:\n",
        "        dir_name = os.path.join(directory, label_type)\n",
        "        for file_path in glob.glob(os.path.join(dir_name, '*.txt')):\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                texts.append(file.read())\n",
        "            labels.append(0 if label_type == 'neg' else 1)\n",
        "    return texts, labels\n",
        "\n",
        "train_texts_imdb, train_labels_imdb = load_data_from_directory('aclImdb/train')\n",
        "test_texts_imdb, test_labels = load_data_from_directory('aclImdb/test')"
      ],
      "metadata": {
        "id": "iSSaPfls1Ri4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize the data and Prepare training Dataset"
      ],
      "metadata": {
        "id": "ItATuR4NWxBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "MAX_SEQ_LEN = 128\n",
        "\n",
        "# Tokenization Part\n",
        "\n",
        "train_texts = [... for text in train_texts_imdb]\n",
        "test_texts = [... for text in test_texts_imdb]"
      ],
      "metadata": {
        "id": "FWJ5qLIEJEVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "train_texts, valid_texts, train_labels, valid_labels = train_test_split(\n",
        "    train_texts, train_labels_imdb, test_size=0.1, random_state=1234)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "train_dataset = TextDataset(train_texts, train_labels)\n",
        "valid_dataset = TextDataset(valid_texts, valid_labels)\n",
        "test_dataset = TextDataset(test_texts, test_labels)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "lJd4pqKq2PvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Model class\n",
        "class TransformerClassifier(nn.Module):\n",
        "    ...\n",
        "\n",
        "vocab_size = ...\n",
        "embed_dim = ...\n",
        "n_heads = ...\n",
        "hidden_dim = ...\n",
        "n_layers = ...\n",
        "output_dim = ...\n",
        "dropout = ...\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TransformerClassifier(...).to(device)"
      ],
      "metadata": {
        "id": "HXRXuDTAJZV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = ...\n",
        "# Train Model and Provide train and validation loss/accuracy\n",
        "..."
      ],
      "metadata": {
        "id": "xLtiX6GsJ4Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test and save the model\n",
        "model.load_state_dict(torch.load('transformer_model.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "FkCExRo3Jik0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PHASE II: Use GPT2 Tokenizer"
      ],
      "metadata": {
        "id": "iU7qA3wgxKmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install tiktoken library\n",
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-YeRYScxTWz",
        "outputId": "9e966097-4fe4-48f0-d5b0-b7d6c040e6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "xOcLHQloxJ2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer with the \"gpt2\" encoding\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "8TqKYxsRxPDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vocabulary size\n",
        "vocab_size = tokenizer.n_vocab\n",
        "print(f\"Vocabulary size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go9cKOUErpeh",
        "outputId": "ac1315f8-ff76-4526-ef1b-97bf51670a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 50257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Text and Visualize Tokens\n",
        "text = \"\"\"Deep learning is the subset of machine learning methods based on neural networks with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised, unsupervised or hayalettiklerimiz.\"\"\"\n",
        "# encode the text into token IDs\n",
        "token_ids = tokenizer.encode(text)\n",
        "print(text)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4DpP7muayE3",
        "outputId": "a16d613b-cf2b-4033-b3cd-c15a7d50b51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep learning is the subset of machine learning methods based on neural networks with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised, unsupervised or hayalettiklerimiz.\n",
            "[29744, 4673, 318, 262, 24637, 286, 4572, 4673, 5050, 1912, 319, 17019, 7686, 351, 10552, 4673, 13, 383, 43441, 366, 22089, 1, 10229, 284, 262, 779, 286, 3294, 11685, 287, 262, 3127, 13, 25458, 973, 460, 307, 2035, 28679, 11, 10663, 12, 16668, 16149, 11, 555, 16668, 16149, 393, 27678, 282, 3087, 1134, 1754, 320, 528, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the tokens themselves\n",
        "tokens = []\n",
        "for token_id in token_ids:\n",
        "    token = tokenizer.decode([token_id])\n",
        "    tokens.append(token)\n",
        "print(f\"All tokens: {tokens}\")\n",
        "print(f\"Last 15 tokens: {tokens[-15:]}\")\n",
        "# to use list comprehension uncomment the following line\n",
        "# tokens = [tokenizer.decode([token_id]) for token_id in token_ids]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQRo0WbfxfxJ",
        "outputId": "45462185-54bf-453f-915b-14e8992bac97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tokens: ['Deep', ' learning', ' is', ' the', ' subset', ' of', ' machine', ' learning', ' methods', ' based', ' on', ' neural', ' networks', ' with', ' representation', ' learning', '.', ' The', ' adjective', ' \"', 'deep', '\"', ' refers', ' to', ' the', ' use', ' of', ' multiple', ' layers', ' in', ' the', ' network', '.', ' Methods', ' used', ' can', ' be', ' either', ' supervised', ',', ' semi', '-', 'super', 'vised', ',', ' un', 'super', 'vised', ' or', ' hay', 'al', 'ett', 'ik', 'ler', 'im', 'iz', '.']\n",
            "Last 15 tokens: ['super', 'vised', ',', ' un', 'super', 'vised', ' or', ' hay', 'al', 'ett', 'ik', 'ler', 'im', 'iz', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Working with a PADding Token\n",
        "- You are going to use your version of encode and decode with pad token methods in your assignment"
      ],
      "metadata": {
        "id": "4eUYws10xXUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncating and Padding Sequences\n",
        "MAX_SEQ_LEN = 10\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "PAD_TOKEN_ID = vocab_size\n",
        "vocab_size = vocab_size + 1\n",
        "# Example text longer than MAX_SEQ_LEN\n",
        "long_text = \"Deep learning methods used can be either supervised, semi-supervised or unsupervised.\"\n",
        "short_text = \"Learning unsupervised learning methods is great.\"\n",
        "\n",
        "# Encode and truncate the text to MAX_SEQ_LEN\n",
        "def truncate_or_pad_token_ids(tokenids: list, max_seq_len, PAD_TOKEN_ID):\n",
        "    # truncate if longer than max_seq_len\n",
        "    tokens2use = tokenids[:max_seq_len]\n",
        "    # if the nuber of tokens is less than max _seq len then do padding (with unknown) !\n",
        "    if len(tokens2use) < max_seq_len:\n",
        "        tokens2use += [PAD_TOKEN_ID] * (max_seq_len - len(tokens2use))\n",
        "    return tokens2use\n",
        "\n",
        "def encode_with_pad_token(tokenizer, text, max_seq_len, PAD_TOKEN_ID):\n",
        "    token_ids = tokenizer.encode(text)\n",
        "    token_ids = truncate_or_pad_token_ids(token_ids, max_seq_len, PAD_TOKEN_ID)\n",
        "    return token_ids\n",
        "\n",
        "def decode_with_pad_token(tokenizer, tokenids, PAD_TOKEN_ID, PAD_TOKEN):\n",
        "    tokens = [tokenizer.decode([token]) if token != PAD_TOKEN_ID else PAD_TOKEN for token in tokenids]\n",
        "    return tokens\n",
        "\n",
        "# LONG TEXT\n",
        "token_ids = encode_with_pad_token(tokenizer, long_text, MAX_SEQ_LEN, PAD_TOKEN_ID)\n",
        "tokens = decode_with_pad_token(tokenizer, token_ids, PAD_TOKEN_ID, PAD_TOKEN)\n",
        "print(\"Long Text:\", long_text)\n",
        "print(\"Truncated Token IDs:\", token_ids)\n",
        "print(\"Truncated Tokens:\", tokens)\n",
        "\n",
        "# SHORT TEXT\n",
        "token_ids = encode_with_pad_token(tokenizer, short_text, MAX_SEQ_LEN, PAD_TOKEN_ID)\n",
        "tokens = decode_with_pad_token(tokenizer, token_ids, PAD_TOKEN_ID, PAD_TOKEN)\n",
        "print(\"Short Text:\", short_text)\n",
        "print(\"Padded Token IDs:\", token_ids)\n",
        "print(\"Padded Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnh8XDj3xnO5",
        "outputId": "2a807415-419e-4d24-a255-2202e97d0eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Long Text: Deep learning methods used can be either supervised, semi-supervised or unsupervised.\n",
            "Truncated Token IDs: [29744, 4673, 5050, 973, 460, 307, 2035, 28679, 11, 10663]\n",
            "Truncated Tokens: ['Deep', ' learning', ' methods', ' used', ' can', ' be', ' either', ' supervised', ',', ' semi']\n",
            "Short Text: Learning unsupervised learning methods is great.\n",
            "Padded Token IDs: [41730, 555, 16668, 16149, 4673, 5050, 318, 1049, 13, 50268]\n",
            "Padded Tokens: ['Learning', ' un', 'super', 'vised', ' learning', ' methods', ' is', ' great', '.', '<PAD>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### DATA PREPARATION with GPT2 TOKENIZER"
      ],
      "metadata": {
        "id": "uDDRIDT_5ouE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data and Prepare training Dataset\n",
        "...\n",
        "\n",
        "# NO NEED for handling UNKNOWNS\n"
      ],
      "metadata": {
        "id": "rBLoqyB551Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training and Evaluation part (similar to phase I)"
      ],
      "metadata": {
        "id": "b5hNTS19wvie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}