{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ILkT4z8zw0t",
        "outputId": "d992d1af-c7eb-413d-a2b3-840e016b235b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.3.0+cu121\n"
          ]
        }
      ],
      "source": [
        "# necessary imports\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np # for transformation\n",
        "\n",
        "import torch\n",
        "import torchvision # load datasets\n",
        "import torchvision.transforms as transforms # transform data\n",
        "import torch.nn as nn # basic building block for neural networks\n",
        "import torch.nn.functional as F # import functions like Relu\n",
        "import torch.optim as optim # optimizer\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omVQuj6vz2S-",
        "outputId": "11401988-c3ae-4d0f-a435-17878f7fb18a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7824e01a8430>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Setting the seed\n",
        "# replace list of student ids (here [34xxx, ...]) with last 5 digits of the student ids of your group members\n",
        "group_studentids = [34670, 34772, 34674]\n",
        "groupid = int(sum(group_studentids)/len(group_studentids))\n",
        "torch.manual_seed(groupid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrn-as2uz6SD"
      },
      "source": [
        "#### Classification for CIFAR-100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzRL1s3rz87V"
      },
      "source": [
        "##### Load the dataset from torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09V0MoQHz3wX",
        "outputId": "f2adf9ea-cdde-4255-d436-98dd8e937126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:05<00:00, 29838513.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Number of training batches: 782\n",
            "Number of testing batches: 157\n",
            "Number of classes: 100\n"
          ]
        }
      ],
      "source": [
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762))\n",
        "])\n",
        "\n",
        "# Set batch_size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 4\n",
        "\n",
        "# Load train data\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "# Load test data\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "# Verify the data loaders\n",
        "print(\"Number of training batches:\", len(trainloader))\n",
        "print(\"Number of testing batches:\", len(testloader))\n",
        "print(f'Number of classes: {len(trainset.classes)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6LDtDqa0cc2"
      },
      "source": [
        "#### Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKF1smZC0YYO"
      },
      "outputs": [],
      "source": [
        "# set model hyperparameters\n",
        "d_in = 3 # rgb - 3 channels\n",
        "d_out = 100\n",
        "activation_fn = nn.ReLU()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIap149k0iiG"
      },
      "outputs": [],
      "source": [
        "# DEFINE NEURAL NETWORK CLASS\n",
        "class ConvNetDA520(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(ConvNetDA520, self).__init__()\n",
        "      # 1. A valid convolution with kernel size 5, 1 input channel and 10 output channels\n",
        "      self.conv1 = nn.Conv2d(in_channels=d_in, out_channels=20, kernel_size=5, padding='valid') # 32x32 => 28x28\n",
        "\n",
        "      # 2. A max pooling operation over a 2x2 area\n",
        "      self.maxpool1 = nn.MaxPool2d(kernel_size=2) # 28x28 => 14x14\n",
        "\n",
        "      # 3. A Relu\n",
        "      self.act_fn = activation_fn\n",
        "\n",
        "      # 4. A valid convolution with kernel size 5, 100 input channels and 20 output channels\n",
        "      self.conv2 = nn.Conv2d(in_channels=20, out_channels=30, kernel_size=5, padding='valid') # 14x14 => 10x10\n",
        "\n",
        "      # 5. A 2D Dropout layer\n",
        "      self.drop = nn.Dropout2d(p=0.5)\n",
        "      self.maxpool2 = nn.MaxPool2d(kernel_size=2) # 10x10 => 5x5\n",
        "\n",
        "      # 6. A fully connected layer mapping from (whatever dimensions we are at-- find out using .shape) to 50\n",
        "      self.fc1 = nn.Linear(in_features=5*5*30, out_features=50)\n",
        "\n",
        "      # 7. A fully connected layer mapping from 500 to 100 dimensions\n",
        "      self.fc2 = nn.Linear(in_features=50, out_features=100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = x.flatten(1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x, dim=1) #softmax + nll / crossentropy #düşünelim\n",
        "        return x\n",
        "# use different convolutional and pooling layer configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKUgPirk0qAT"
      },
      "source": [
        "##### Create and Train CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3JfOIBzW75c"
      },
      "outputs": [],
      "source": [
        "model = ConvNetDA520()\n",
        "\n",
        "# Initialize model weights\n",
        "def weights_init(layer_in):\n",
        "  if isinstance(layer_in, nn.Linear):\n",
        "    nn.init.kaiming_uniform_(layer_in.weight)\n",
        "    layer_in.bias.data.fill_(0.0)\n",
        "\n",
        "model.apply(weights_init)\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Main training routine\n",
        "def train(epochs, model):\n",
        "  losses_train = np.zeros((epochs))\n",
        "  errors_train = np.zeros((epochs))\n",
        "  losses_val = np.zeros((epochs))\n",
        "  errors_val = np.zeros((epochs))\n",
        "  accuracy_train = np.zeros((epochs))\n",
        "  accuracy_val =np.zeros((epochs))\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    train_correct = 0 #for accuracy\n",
        "    train_loss = 0\n",
        "    val_correct = 0 #for accuracy\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data, target in trainloader:\n",
        "        output = model(data)\n",
        "        train_loss += loss_fn(output, target).item()\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        train_correct += (predicted == target).sum().item()\n",
        "\n",
        "      for data, target in testloader:\n",
        "        output = model(data)\n",
        "        val_loss += loss_fn(output, target).item()\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    train_accuracy = 100. * train_correct / len(trainloader.dataset)\n",
        "    val_accuracy = 100. * val_correct / len(testloader.dataset)\n",
        "\n",
        "    errors_train[epoch] = 100 - train_accuracy\n",
        "    errors_val[epoch] = 100 - val_accuracy\n",
        "    losses_train[epoch] = train_loss\n",
        "    losses_val[epoch] = val_loss\n",
        "    accuracy_train[epoch] = train_accuracy\n",
        "    accuracy_val[epoch] = val_accuracy\n",
        "\n",
        "    print(f'Epoch {epoch:5d}, train loss {train_loss:.6f}, train accuracy {accuracy_train[epoch]:3.2f}, val loss {val_loss:.6f}, val error {errors_val[epoch]:3.2f}')\n",
        "\n",
        "  return model, errors_train, errors_val, losses_train, losses_val, accuracy_train, accuracy_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7ix49SRbx78",
        "outputId": "7571c66a-bb08-46ac-b8b6-969b2529b544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch     0, train loss 3106.499215, train accuracy 10.65, val loss 627.170772, val error 89.59\n",
            "Epoch     1, train loss 2896.995012, train accuracy 14.25, val loss 587.498171, val error 86.10\n",
            "Epoch     2, train loss 2754.067101, train accuracy 17.55, val loss 562.538866, val error 83.47\n",
            "Epoch     3, train loss 2682.284593, train accuracy 19.69, val loss 549.041087, val error 81.91\n",
            "Epoch     4, train loss 2561.250432, train accuracy 21.94, val loss 526.668368, val error 79.71\n",
            "Epoch     5, train loss 2519.549774, train accuracy 23.06, val loss 520.072831, val error 78.92\n",
            "Epoch     6, train loss 2443.976754, train accuracy 25.78, val loss 506.739607, val error 76.67\n",
            "Epoch     7, train loss 2331.531075, train accuracy 27.96, val loss 488.377648, val error 74.73\n",
            "Epoch     8, train loss 2293.859428, train accuracy 28.97, val loss 482.132123, val error 74.08\n",
            "Epoch     9, train loss 2246.237526, train accuracy 30.52, val loss 474.017062, val error 72.49\n",
            "Epoch    10, train loss 2248.136276, train accuracy 30.60, val loss 476.548517, val error 73.15\n",
            "Epoch    11, train loss 2186.703153, train accuracy 31.85, val loss 465.930545, val error 71.93\n",
            "Epoch    12, train loss 2158.179484, train accuracy 32.11, val loss 461.809150, val error 71.67\n",
            "Epoch    13, train loss 2141.065647, train accuracy 33.20, val loss 459.619567, val error 71.15\n",
            "Epoch    14, train loss 2106.672615, train accuracy 33.92, val loss 454.892659, val error 70.93\n",
            "Epoch    15, train loss 2105.714995, train accuracy 34.34, val loss 455.352230, val error 70.32\n",
            "Epoch    16, train loss 2080.365190, train accuracy 34.92, val loss 450.974804, val error 69.96\n",
            "Epoch    17, train loss 2065.005383, train accuracy 34.89, val loss 450.330782, val error 70.25\n",
            "Epoch    18, train loss 2063.289983, train accuracy 35.27, val loss 450.084091, val error 70.09\n",
            "Epoch    19, train loss 2013.454557, train accuracy 35.91, val loss 442.334159, val error 69.80\n",
            "Epoch    20, train loss 2017.180463, train accuracy 36.45, val loss 443.315547, val error 69.26\n",
            "Epoch    21, train loss 2015.142347, train accuracy 35.96, val loss 443.502481, val error 69.90\n",
            "Epoch    22, train loss 1989.873162, train accuracy 36.84, val loss 440.417684, val error 69.26\n",
            "Epoch    23, train loss 1971.320446, train accuracy 37.24, val loss 437.191464, val error 69.06\n",
            "Epoch    24, train loss 1965.471625, train accuracy 36.72, val loss 436.811980, val error 69.05\n",
            "Epoch    25, train loss 1972.144180, train accuracy 37.03, val loss 438.813680, val error 69.50\n",
            "Epoch    26, train loss 1935.488292, train accuracy 37.86, val loss 433.344457, val error 68.32\n",
            "Epoch    27, train loss 1960.968371, train accuracy 37.44, val loss 438.413975, val error 69.23\n",
            "Epoch    28, train loss 1942.127322, train accuracy 37.71, val loss 434.411977, val error 68.75\n",
            "Epoch    29, train loss 1947.547395, train accuracy 37.96, val loss 437.577829, val error 68.85\n",
            "Epoch    30, train loss 1924.244137, train accuracy 38.31, val loss 432.549268, val error 68.11\n",
            "Epoch    31, train loss 1929.484701, train accuracy 38.21, val loss 435.244247, val error 68.87\n",
            "Epoch    32, train loss 1924.875223, train accuracy 38.62, val loss 434.239932, val error 68.21\n",
            "Epoch    33, train loss 1932.876591, train accuracy 37.81, val loss 438.011584, val error 68.95\n",
            "Epoch    34, train loss 1930.410277, train accuracy 38.08, val loss 435.355362, val error 68.85\n",
            "Epoch    35, train loss 1888.115752, train accuracy 39.97, val loss 430.556303, val error 68.03\n",
            "Epoch    36, train loss 1894.483418, train accuracy 39.75, val loss 432.198421, val error 68.48\n",
            "Epoch    37, train loss 1880.203120, train accuracy 40.13, val loss 429.231779, val error 68.12\n",
            "Epoch    38, train loss 1901.859063, train accuracy 38.44, val loss 434.518921, val error 69.04\n",
            "Epoch    39, train loss 1895.564483, train accuracy 39.00, val loss 434.084853, val error 69.05\n",
            "Epoch    40, train loss 1877.140164, train accuracy 39.27, val loss 432.636719, val error 68.85\n",
            "Epoch    41, train loss 1886.098423, train accuracy 38.95, val loss 433.276043, val error 69.10\n",
            "Epoch    42, train loss 1847.872890, train accuracy 40.53, val loss 426.273369, val error 67.71\n",
            "Epoch    43, train loss 1866.241794, train accuracy 39.95, val loss 431.870615, val error 68.61\n",
            "Epoch    44, train loss 1847.569780, train accuracy 40.18, val loss 427.706491, val error 68.45\n",
            "Epoch    45, train loss 1840.952377, train accuracy 40.23, val loss 426.382430, val error 67.88\n",
            "Epoch    46, train loss 1830.738063, train accuracy 41.02, val loss 425.912803, val error 67.53\n",
            "Epoch    47, train loss 1823.861987, train accuracy 40.98, val loss 424.134965, val error 67.82\n",
            "Epoch    48, train loss 1823.566357, train accuracy 41.15, val loss 425.255966, val error 68.21\n",
            "Epoch    49, train loss 1824.218098, train accuracy 41.16, val loss 426.478412, val error 68.05\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "epochs = 50\n",
        "\n",
        "trained_model, errors_train, errors_val, losses_train, losses_val, accuracy_train, accuracy_val = train(epochs, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "fiPKShs3bryJ",
        "outputId": "7dd8d03c-f170-41b3-823e-81418343fe03"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZlElEQVR4nO3deVhUZcMG8HvYhn1HFkVFxT03VEQzNxSX3FMpKi1fe1XctTdbXCu1LFNzyxZscUsNtbJMSXEjJc1dyX0HVGQQkXWe74/nm4GRRdCBAc79u65zMXPmzJln5oBz+6wqIYQAERERkYKZmboARERERKbGQERERESKx0BEREREisdARERERIrHQERERESKx0BEREREisdARERERIrHQERERESKx0BEREREisdARESPdfnyZahUKqxatUq/b+bMmVCpVMV6vkqlwsyZM41apo4dO6Jjx45GPWdlUbNmTQwbNszUxSCqUBiISNFWrVoFlUql36ytrVG3bl2MGTMGCQkJRnudtLQ0zJw5E7t37y7W8bt374ZKpcLGjRtL/Fp9+vSBra0t7t+/X+gxYWFhsLKywt27d0t8/rJ0+vRpzJw5E5cvXzZ1UfR010a3mZubo0qVKnjhhRdw5swZUxevQE/6Oe7cuROdO3eGk5MTHBwcEBAQgPXr1xsck56ejrlz56Jhw4awtbVF1apVMWjQIJw6dapYr6HVavHxxx/Dz88P1tbWaNKkCdauXZvvuGHDhhl87rqtfv36JXpPRIWxMHUBiMqD2bNnw8/PD+np6di3bx+WL1+Obdu24eTJk7C1tX3q86elpWHWrFkAUOq1GmFhYfj5558RGRmJV199tcCybNmyBd27d4ebm9sTv857772HqVOnPk1RH+v06dOYNWsWOnbsiJo1axo89scff5Tqaz/OuHHj0KpVK2RlZeH48eNYsWIFdu/ejZMnT8LLy8ukZXtUUZ9jYSIiIjB8+HB07doVc+bMgbm5OeLi4nDt2jWD48LCwrB161aMGDECLVq0wM2bN7F06VIEBQXhxIkTqFGjRpGv8+6772LevHkYMWIEWrVqhS1btuCll16CSqVCaGiowbFqtRpfffWVwT4nJ6divR+ix2EgIgLQo0cPtGzZEgDwn//8B25ubliwYAG2bNmCF1988YnPq9VqkZmZaaxiFkufPn3g4OCANWvWFBiItmzZggcPHiAsLOypXsfCwgIWFqb7J8TKyspkrw0A7du3xwsvvKC/X69ePYwaNQrfffcd/ve//5mwZE/v8uXLCA8Px9ixY7Fo0aJCj7tx4wZ++uknTJkyBfPnz9fvb9++PTp37oyffvoJEydOLPL5n376KcLDw7FkyRIA8u+vQ4cOePPNNzFo0CCYm5vrj7ewsMDLL79shHdIlB+bzIgK0LlzZwDApUuXAACffPIJ2rZtCzc3N9jY2CAgIKDA5iyVSoUxY8Zg9erVaNSoEdRqNVasWAEPDw8AwKxZs/RV/U/Sp+bs2bO4evVqkcfY2NhgwIABiIqKQmJiYr7H16xZAwcHB/Tp0wdJSUmYMmUKnnnmGdjb28PR0RE9evTAsWPHHluWgvoQZWRkYOLEifDw8NC/xvXr1/M998qVKxg9ejTq1asHGxsbuLm5YdCgQQZNOqtWrcKgQYMAAJ06ddJ/brpmx4L6ECUmJmL48OHw9PSEtbU1mjZtim+//dbgGF1/qE8++QQrV65E7dq1oVar0apVK8TGxj72fRemffv2AIALFy4Y7L9x4wZef/11eHp6Qq1Wo1GjRvjmm2/yPf/zzz9Ho0aNYGtrCxcXF7Rs2RJr1qzRPz5s2LACa3ce15frcZ9jQVasWIGcnBzMnj0bAJCamgohRL7jdM2ynp6eBvu9vb0ByN/FomzZsgVZWVkYPXq0fp9KpcKoUaNw/fp1xMTE5HtOTk4OUlJSijwv0ZNgICIqgO5LTdektGjRIjRv3hyzZ8/GnDlzYGFhgUGDBuHXX3/N99w///wTEydOxJAhQ7Bo0SK0atUKy5cvBwD0798f33//Pb7//nsMGDCgxOVq0KBBgbU+jwoLC0N2djZ+/PFHg/1JSUnYvn07+vfvDxsbG1y8eBGbN2/G888/jwULFuDNN9/EiRMn0KFDB9y8ebPE5fvPf/6DhQsXolu3bpg3bx4sLS3Rq1evfMfFxsbiwIEDCA0NxeLFizFy5EhERUWhY8eOSEtLAwA899xzGDduHADgnXfe0X9uDRo0KPC1Hz58iI4dO+L7779HWFgY5s+fDycnJwwbNqzAWo41a9Zg/vz5+O9//4sPPvgAly9fxoABA5CVlVXi9w1AH+ZcXFz0+xISEtCmTRvs3LkTY8aMwaJFi1CnTh0MHz4cCxcu1B/35ZdfYty4cWjYsCEWLlyIWbNmoVmzZjh48OATlSWvkn6OgOw7VL9+fWzbtg3VqlWDg4MD3NzcMG3aNGi1Wv1xtWvXRrVq1fDpp5/i559/xvXr13Ho0CGMHDkSfn5++Zq8HvXPP//Azs4uX1lat26tfzyvtLQ0ODo6wsnJCa6urggPD0dqamqJPg+iQgkiBYuIiBAAxM6dO8Xt27fFtWvXxLp164Sbm5uwsbER169fF0IIkZaWZvC8zMxM0bhxY9G5c2eD/QCEmZmZOHXqlMH+27dvCwBixowZxSrXrl27BACxYcOGfOfv0KHDY5+fnZ0tvL29RVBQkMH+FStWCABi+/btQggh0tPTRU5OjsExly5dEmq1WsyePdtgHwARERGh3zdjxgyR95+Qo0ePCgBi9OjRBud76aWX8r33Rz9PIYSIiYkRAMR3332n37dhwwYBQOzatSvf8R06dDD4LBYuXCgAiB9++EG/LzMzUwQFBQl7e3uRkpJi8F7c3NxEUlKS/tgtW7YIAOLnn3/O91p56a7NN998I27fvi1u3rwpfv/9d1GnTh2hUqnEoUOH9McOHz5ceHt7izt37hicIzQ0VDg5Oek/h759+4pGjRoV+bpDhw4VNWrUyLf/0esghBA1atQQQ4cO1d8v6nMsiKOjo3BxcRFqtVpMmzZNbNy4UX8dp06danDswYMHRe3atQUA/RYQECBu3br12Nfp1auXqFWrVr79Dx48yPdaU6dOFW+99ZZYv369WLt2rRg6dKgAINq1ayeysrKK9b6IisIaIiIAwcHB8PDwgK+vL0JDQ2Fvb4/IyEhUrVoVgGHV/71796DRaNC+fXscOXIk37k6dOiAhg0blko5hRDFGqlmbm6O0NBQxMTEGDRDrVmzBp6enujSpQsA2UnVzEz+M5CTk4O7d+/C3t4e9erVK/C9FWXbtm0AoK+N0JkwYUK+Y/N+nllZWbh79y7q1KkDZ2fnEr9u3tf38vIy6PNlaWmJcePGITU1FdHR0QbHDxkyxKA2R9fkdfHixWK93uuvvw4PDw/4+Pige/fu0Gg0+P7779GqVSsA8lpt2rQJvXv3hhACd+7c0W8hISHQaDT69+rs7Izr168/VZOdMaWmpuLevXuYNWsWZs+ejYEDB2L16tXo3r07Fi1aZDCC0cXFBc2aNcPUqVOxefNmfPLJJ7h8+TIGDRqE9PT0Il/n4cOHUKvV+fZbW1vrH9eZO3cu5s2bh8GDByM0NBSrVq3Chx9+iP379z/RaEyiRzEQEQFYunQpduzYgV27duH06dO4ePEiQkJC9I//8ssvaNOmDaytreHq6goPDw8sX74cGo0m37n8/PzKsuiF0nWa1vVDuX79Ovbu3YvQ0FB9R1WtVovPPvsM/v7+UKvVcHd3h4eHB44fP17geyvKlStXYGZmhtq1axvsr1evXr5jHz58iOnTp8PX19fgdZOTk0v8unlf39/fXx/wdHTNMVeuXDHYX716dYP7unB07969Yr3e9OnTsWPHDv1oPo1GY/Dat2/fRnJyMlauXAkPDw+D7bXXXgMAfR+vt956C/b29mjdujX8/f0RHh6O/fv3l+DdG5cusD46oODFF1/Ew4cP9U1Zuv8YBAUFYe7cuejbty8mT56MTZs2Yd++fYiIiHjs62RkZOTbrwtSj+uDNHHiRJiZmWHnzp3Ffm9EheEoMyLIPgu6UWaP2rt3L/r06YPnnnsOy5Ytg7e3NywtLREREWHQ6VXncf+Il5WAgADUr18fa9euxTvvvIO1a9dCCGEwumzOnDmYNm0aXn/9dbz//vtwdXWFmZkZJkyYYNBXxNjGjh2LiIgITJgwAUFBQXByctIPsy7N180r7+ilvEQBnYcL8swzzyA4OBgA0K9fP6SlpWHEiBF49tln4evrq38fL7/8MoYOHVrgOZo0aQJAhra4uDj88ssv+P3337Fp0yYsW7YM06dP10/XUFjH6ZycnGKVtyR8fHxw7ty5fJ2lq1SpAiA3NG7atAkJCQno06ePwXEdOnSAo6Mj9u/fj1GjRhX6Ot7e3ti1axeEEAbv79atW/pyFEXXIT8pKan4b46oEAxERI+xadMmWFtbY/v27QbV+4/7329exZ3R2djCwsIwbdo0HD9+HGvWrIG/v7++SQcANm7ciE6dOuHrr782eF5ycjLc3d1L9Fo1atSAVqvFhQsXDGqF4uLi8h27ceNGDB06FJ9++ql+X3p6OpKTkw2OK8nnVqNGDRw/fhxardagpubs2bP6x0vTvHnzEBkZiQ8//FA/stDBwQE5OTn64FQUOzs7DBkyBEOGDEFmZiYGDBiADz/8EG+//Tasra3h4uKS7/MB8td8FaSkv38BAQE4d+4cbty4gVq1aun36zra60ZN6iYvfTSUCSGQk5OD7OzsIl+nWbNm+Oqrr3DmzBmDZmZdZ/JmzZoV+fz79+/jzp07+vIQPQ02mRE9hrm5OVQqlcE/+pcvX8bmzZuLfQ7d5I4FfaGVRHGG3eelqw2aPn06jh49mm/uIXNz83w1Ihs2bMCNGzdKXLYePXoAABYvXmywP+9oqqJe9/PPP8/3xWpnZwegeJ9bz549ER8fbzCTcnZ2Nj7//HPY29ujQ4cOxXkbT6x27doYOHAgVq1ahfj4eJibm2PgwIHYtGkTTp48me/427dv628/OmO4lZUVGjZsCCGEftRb7dq1odFocPz4cf1xt27dQmRk5GPLVpLPEZD9qwAYBGWtVouIiAi4uroiICAAAFC3bl0AwLp16wyev3XrVjx48ADNmzfX79NoNDh79qxBk2jfvn1haWmJZcuW6fcJIbBixQpUrVoVbdu2BSDDckEzr7///vsQQqB79+7Fel9ERWENEdFj9OrVCwsWLED37t3x0ksvITExEUuXLkWdOnUMvpyKYmNjg4YNG2L9+vWoW7cuXF1d0bhxYzRu3LhEZWnQoAE6dOhQ7CVA/Pz80LZtW2zZsgUA8gWi559/HrNnz8Zrr72Gtm3b4sSJE1i9erVBrUBxNWvWDC+++CKWLVsGjUaDtm3bIioqCufPn8937PPPP4/vv/8eTk5OaNiwIWJiYrBz5858M2c3a9YM5ubm+Oijj6DRaKBWq9G5c2d9001eb7zxBr744gsMGzYMhw8fRs2aNbFx40bs378fCxcuhIODQ4nfU0m9+eab+PHHH7Fw4ULMmzcP8+bNw65duxAYGIgRI0agYcOGSEpKwpEjR7Bz5059U0+3bt3g5eWFdu3awdPTE2fOnMGSJUvQq1cvfblDQ0Px1ltvoX///hg3bhzS0tKwfPly1K1b97Ed0UvyOQIyqHTp0gVz587FnTt30LRpU2zevBn79u3DF198oa8p7d27Nxo1aoTZs2fjypUraNOmDc6fP48lS5bA29sbw4cP158zMjISr732GiIiIvTrrFWrVg0TJkzA/PnzkZWVhVatWmHz5s3Yu3cvVq9erW/WjI+PR/PmzfHiiy/ql+rYvn07tm3bhu7du6Nv375PftGIdEwzuI2ofNANu4+NjS3yuK+//lr4+/sLtVot6tevLyIiIgoc7gxAhIeHF3iOAwcOiICAAGFlZfXYIfhPO+w+r6VLlwoAonXr1vkeS09PF5MnTxbe3t7CxsZGtGvXTsTExOQb0l6cYfdCCPHw4UMxbtw44ebmJuzs7ETv3r3FtWvX8r3fe/fuiddee024u7sLe3t7ERISIs6ePZtvuLgQQnz55ZeiVq1awtzc3GDo+KNlFEKIhIQE/XmtrKzEM888Y1DmvO9l/vz5+T6Px10XIQq/NjodO3YUjo6OIjk5WV+m8PBw4evrKywtLYWXl5fo0qWLWLlypf45X3zxhXjuueeEm5ubUKvVonbt2uLNN98UGo3G4Nx//PGHaNy4sbCyshL16tUTP/zwQ7GG3QtR+OdYmPv374vx48cLLy8v/WeZd0oDnaSkJDFx4kRRt25doVarhbu7uwgNDRUXL140OE73t/bo9cjJyRFz5swRNWrUEFZWVqJRo0b5XufevXvi5ZdfFnXq1BG2trZCrVaLRo0aiTlz5ojMzMwi3wdRcamEKGYPQiIiIqJKin2IiIiISPEYiIiIiEjxGIiIiIhI8UwaiPbs2YPevXvDx8cHKpUq3zBmIQSmT58Ob29v2NjYIDg4GOfOnTM4JikpCWFhYXB0dISzszOGDx/Oxf6IiIioREwaiB48eICmTZti6dKlBT7+8ccfY/HixVixYgUOHjwIOzs7hISEGKyPExYWhlOnTmHHjh345ZdfsGfPHrzxxhtl9RaIiIioEig3o8xUKhUiIyPRr18/ALJ2yMfHB5MnT8aUKVMAyIm9PD09sWrVKoSGhupnN42NjdUvu/D777+jZ8+euH79+mOnfSciIiICyvHEjJcuXUJ8fLzBlPdOTk4IDAxETEyMfiVvZ2dngzWogoODYWZmhoMHD6J///4FnjsjI8NgQUGtVoukpCS4ubmZbIkFIiIiKhkhBO7fvw8fH598CzuXVLkNRPHx8QCQb3FBT09P/WPx8fH5Zlq1sLCAq6ur/piCzJ07V79gIhEREVVs165dQ7Vq1Z7qHOU2EJWmt99+G5MmTdLf12g0qF69Oq5duwZHR0cTloyIiIiKKyUlBb6+vkZZmqfcBiIvLy8AcjVlb29v/f6EhAT9CsheXl5ITEw0eF52djaSkpL0zy+IWq02WLVcx9HRkYGIiIiogjFGd5dyOw+Rn58fvLy8EBUVpd+XkpKCgwcPIigoCAAQFBSE5ORkHD58WH/Mn3/+Ca1Wi8DAwDIvMxEREVVMJq0hSk1NNVgJ+9KlSzh69ChcXV1RvXp1TJgwAR988AH8/f3h5+eHadOmwcfHRz8SrUGDBujevTtGjBiBFStWICsrC2PGjEFoaChHmBEREVGxmTQQ/f333+jUqZP+vq5fz9ChQ7Fq1Sr873//w4MHD/DGG28gOTkZzz77LH7//XdYW1vrn7N69WqMGTMGXbp0gZmZGQYOHIjFixeX+XshIiKiiqvczENkSikpKXBycoJGo2EfIiKiCi4nJwdZWVmmLgYZgaWlJczNzQt93Jjf3+W2UzUREVFJCCEQHx+P5ORkUxeFjMjZ2RleXl6lPk8gAxEREVUKujBUpUoV2NracqLdCk4IgbS0NP1o8rwjzksDAxEREVV4OTk5+jDk5uZm6uKQkdjY2AAAEhMTUaVKlSKbz55WuR12T0REVFy6PkO2trYmLgkZm+6alna/MAYiIiKqNNhMVvmU1TVlICIiIiLFYyAiIiKqJGrWrImFCxeauhgVEjtVExERmVDHjh3RrFkzowSZ2NhY2NnZPX2hFIiBiIiIqBwTQiAnJwcWFo//yvbw8CiDElVObDIjIiIykWHDhiE6OhqLFi2CSqWCSqXCqlWroFKp8NtvvyEgIABqtRr79u3DhQsX0LdvX3h6esLe3h6tWrXCzp07Dc73aJOZSqXCV199hf79+8PW1hb+/v7YunVrGb/LioGBiIiIKichgAcPyn4rwYpYixYtQlBQEEaMGIFbt27h1q1b8PX1BQBMnToV8+bNw5kzZ9CkSROkpqaiZ8+eiIqKwj///IPu3bujd+/euHr1apGvMWvWLAwePBjHjx9Hz549ERYWhqSkpKf6aCsjNpkREVHllJYG2NuX/eumpgLF7Mfj5OQEKysr2NrawsvLCwBw9uxZAMDs2bPRtWtX/bGurq5o2rSp/v7777+PyMhIbN26FWPGjCn0NYYNG4YXX3wRADBnzhwsXrwYhw4dQvfu3Uv81ioz1hARERGVQy1btjS4n5qaiilTpqBBgwZwdnaGvb09zpw589gaoiZNmuhv29nZwdHRUb8cBuViDREREVVOtraytsYUr2sEj44WmzJlCnbs2IFPPvkEderUgY2NDV544QVkZmYWeR5LS0uD+yqVClqt1ihlrEwYiIiIqHJSqYrddGVKVlZWyMnJeexx+/fvx7Bhw9C/f38Assbo8uXLpVw65WCTGRERkQnVrFkTBw8exOXLl3Hnzp1Ca2/8/f3x008/4ejRozh27Bheeukl1vQYEQMRERGRCU2ZMgXm5uZo2LAhPDw8Cu0TtGDBAri4uKBt27bo3bs3QkJC0KJFizIubeWlEqIE4wMrqZSUFDg5OUGj0cDR0dHUxSEiohJKT0/HpUuX4OfnB2tra1MXh4yoqGtrzO9v1hARERGR4jEQERERkeIxEBEREZHiMRARERGR4jEQERERkeIxEBEREZHiMRARERGR4jEQERERkeIxEBEREZHiMRARERFVYDVr1sTChQv191UqFTZv3lzo8ZcvX4ZKpcLRo0ef6nWNdZ7ygqvdExERVSK3bt2Ci4uLUc85bNgwJCcnGwQtX19f3Lp1C+7u7kZ9LVNhICIiIqpEvLy8yuR1zM3Ny+y1ygKbzIiIiExk5cqV8PHxgVarNdjft29fvP7667hw4QL69u0LT09P2Nvbo1WrVti5c2eR53y0yezQoUNo3rw5rK2t0bJlS/zzzz8Gx+fk5GD48OHw8/ODjY0N6tWrh0WLFukfnzlzJr799lts2bIFKpUKKpUKu3fvLrDJLDo6Gq1bt4ZarYa3tzemTp2K7Oxs/eMdO3bEuHHj8L///Q+urq7w8vLCzJkzS/7BlQLWEOVx/z7Axe6JiCoHIYC0tLJ/XVtbQKUq3rGDBg3C2LFjsWvXLnTp0gUAkJSUhN9//x3btm1DamoqevbsiQ8//BBqtRrfffcdevfujbi4OFSvXv2x509NTcXzzz+Prl274ocffsClS5cwfvx4g2O0Wi2qVauGDRs2wM3NDQcOHMAbb7wBb29vDB48GFOmTMGZM2eQkpKCiIgIAICrqytu3rxpcJ4bN26gZ8+eGDZsGL777jucPXsWI0aMgLW1tUHo+fbbbzFp0iQcPHgQMTExGDZsGNq1a4euXbsW70MrJQxEeYx9IwObfin+LzIREZVfaWmAvX3Zv25qKmBnV7xjXVxc0KNHD6xZs0YfiDZu3Ah3d3d06tQJZmZmaNq0qf74999/H5GRkdi6dSvGjBnz2POvWbMGWq0WX3/9NaytrdGoUSNcv34do0aN0h9jaWmJWbNm6e/7+fkhJiYGP/74IwYPHgx7e3vY2NggIyOjyCayZcuWwdfXF0uWLIFKpUL9+vVx8+ZNvPXWW5g+fTrMzGSjVJMmTTBjxgwAgL+/P5YsWYKoqCiTByI2meURuU2NzxdrH38gERGRkYSFhWHTpk3IyMgAAKxevRqhoaEwMzNDamoqpkyZggYNGsDZ2Rn29vY4c+YMrl69WqxznzlzBk2aNIG1tbV+X1BQUL7jli5dioCAAHh4eMDe3h4rV64s9mvkfa2goCCo8tQqtGvXDqmpqbh+/bp+X5MmTQye5+3tjcTExBK9VmlgDdEjJk8SaNUaKOD3hYiIKhBbW1lbY4rXLYnevXtDCIFff/0VrVq1wt69e/HZZ58BAKZMmYIdO3bgk08+QZ06dWBjY4MXXngBmZmZRivvunXrMGXKFHz66acICgqCg4MD5s+fj4MHDxrtNfKytLQ0uK9SqfL1oTIFBqI8+mMTIrWvYXC/DBw5qYaHh6lLRERET0qlKn7TlSlZW1tjwIABWL16Nc6fP4969eqhRYsWAID9+/dj2LBh6N+/PwDZJ+jy5cvFPneDBg3w/fffIz09XV9L9Ndffxkcs3//frRt2xajR4/W77tw4YLBMVZWVsjJyXnsa23atAlCCH0t0f79++Hg4IBq1aoVu8ymwiazPD7vsxP1cBbXE9V4aVAWHnPtiYiIjCIsLAy//vorvvnmG4SFhen3+/v746effsLRo0dx7NgxvPTSSyWqTXnppZegUqkwYsQInD59Gtu2bcMnn3xicIy/vz/+/vtvbN++Hf/++y+mTZuG2NhYg2Nq1qyJ48ePIy4uDnfu3EFWVla+1xo9ejSuXbuGsWPH4uzZs9iyZQtmzJiBSZMm6fsPlWflv4RlyGHpR9jkOxG2eICd0ZaYNVOYukhERKQAnTt3hqurK+Li4vDSSy/p9y9YsAAuLi5o27YtevfujZCQEH3tUXHY29vj559/xokTJ9C8eXO8++67+OijjwyO+e9//4sBAwZgyJAhCAwMxN27dw1qiwBgxIgRqFevHlq2bAkPDw/s378/32tVrVoV27Ztw6FDh9C0aVOMHDkSw4cPx3vvvVfCT8M0VEIIxX/rp6SkwMnJCRqNBo4XLmB1q4V4OedbAMC2bUCPHiYuIBERFSk9PR2XLl2Cn5+fQQdiqviKurYG399POW8Oa4ge1bw5whYHYiSWAwBeDs3GlSsmLhMRERGVKgaigowahYX996AlYpGUYoFBA7Lx/6MhiYiIqBJiICqISgV1xAps8J0MFyQh9ogFJk1UfMsiERFRpcVAVBgnJ9T8aQF+MB8GAFi2XIU1a0xbJCIiIiodDERFadkSPRcE4z28DwAYMTwHJ0+auExERFQojhOqfMrqmjIQPc7YsZjZ7xi6YCfS0s0R3EWL06dNXSgiIspLN/txmilWc6VSpbumj85wbWycqfpxVCqYf/Ml1h3ugi7XInA8sSk6dhSIilLhmWdMXTgiIgIAc3NzODs769fEsrW1NVhTiyoeIQTS0tKQmJgIZ2dnmJubl+rrMRAVh4sL3Dd9gT/bdUfXrF/xz+0W6NQJ2LkTaNbM1IUjIiIA+pXYy8NCoWQ8zs7O+mtbmjgxI0owsdO33+LesAkIwXbEojVcXIAdO4CAgLIrKxERFS0nJ6fApSWo4rG0tCyyZsiYEzOyhqgkhg6Fy9Gj2LGwK3qYbUfMvTbo0gXYvh0IDDR14YiICJDNZ6XdvEKVDztVl9T8+XDq0grbtV3xrDoWGg3QtStw4ICpC0ZERERPioGopCwsgPXr4eDngd8yOqGj81Hcvw+EhAB795q6cERERPQkGIiehJsbsGUL7O2AX5PbIrh6HFJTge7dgV27TF04IiIiKikGoif1zDPAd9/BFg+x9WozdH/mOtLSgF69ZEdrIiIiqjgYiJ7GgAHA9OmwQToi4xqhV7t7ePgQ6N0bWLECyMkxdQGJiIioOBiIntaMGUDfvrDOTMFPF5ujX/d0ZGQAo0bJ4fh79pi6gERERPQ4DERPy8wM+P57oFEjWN26gg33grF4QRacnYFjx4AOHYAhQ4ArV0xdUCIiIioMA5ExODgAmzcDzs6wOLgfY0/8F+f+FRg5UualH38E6tcHZs4EuMwOERFR+cNAZCx16gDr18sEFBEB9/lvYfmSHBw5Ajz3HJCeDsyaBTRoIAMS5wcnIiIqPxiIjKlbN2DBAnl7/nygZ0809U3C7t0yBFWvDly9KpvQOnYEjh41YVmJiIhIj4HI2MaPB9auBWxsgD/+AFq2hOr4MQwaBJw5I2uJbGxkZ+uAAGDOHNYWERERmRoDUWkIDQX++guoVQu4dAkICgLWroWtLTB9OnD2LDB4MKDVAu++C7z4IvsWERERmRIDUWlp0gSIjZVrejx8CLz0EjB5MpCdjerVZXejFSv0K4GgfXvg2jVTF5qIiEiZGIhKk6sr8OuvwNtvy/sLFsiAdPs2AOC//wWiogB3d+DIEaBVKyAmxoTlJSIiUqhyHYhycnIwbdo0+Pn5wcbGBrVr18b7778PkafTjRAC06dPh7e3N2xsbBAcHIxz586ZsNSPMDeXHYU2bgTs7IA//wRatpQJCHIEWmysrFBKSJCdrVetMmmJiYiIFKdcB6KPPvoIy5cvx5IlS3DmzBl89NFH+Pjjj/H555/rj/n444+xePFirFixAgcPHoSdnR1CQkKQnp5uwpIXYOBA4OBBwN9fDjVr1w747jsAQM2awP79ciWQzEzgtdeASZOA7GzTFpmIiEgpVEKU3zFOzz//PDw9PfH111/r9w0cOBA2Njb44YcfIISAj48PJk+ejClTpgAANBoNPD09sWrVKoSGhhbrdVJSUuDk5ASNRgNHR8dSeS96ycnAK68Av/wi70+aBHz8MWBuDq0WmD1bjkQD5Cj+desAF5fSLRIREVFFZMzv73JdQ9S2bVtERUXh33//BQAcO3YM+/btQ48ePQAAly5dQnx8PIKDg/XPcXJyQmBgIGKK6IyTkZGBlJQUg63MODsDW7bI4WaA7FfUrx9w/z7MzORs1hs2ALa2ctR+YKAclUZERESlp1wHoqlTpyI0NBT169eHpaUlmjdvjgkTJiAsLAwAEB8fDwDw9PQ0eJ6np6f+sYLMnTsXTk5O+s3X17f03kRBzMxkNdD69YC1tawtatdOv+DZCy/IJrTq1YFz52Rn66FD5eSOycllW1QiIiIlKNeB6Mcff8Tq1auxZs0aHDlyBN9++y0++eQTfPvtt0913rfffhsajUa/XTPVePfBg4HoaMDLCzhxAmjdGjhwAADQrJnsbP3cc0BqquxuNGQI4OEBdOoEfPKJnOix/DZ4EhERVRzlOhC9+eab+lqiZ555Bq+88gomTpyIuXPnAgC8vLwAAAkJCQbPS0hI0D9WELVaDUdHR4PNZFq3lsmnWTMgMVGmnR9+AABUqSIHpe3aBUyZItdBy84Gdu8G3nwTaNhQLqE2bhywfbtcL42IiIhKrlwHorS0NJiZGRbR3NwcWq0WAODn5wcvLy9ERUXpH09JScHBgwcRFBRUpmV9KtWqAfv2yb5EmZmy0/W77wJaLczN5VD8+fOB06eBCxeAxYtlh2srK+DiReDzz4Hu3QEfHzntEREREZVMuQ5EvXv3xocffohff/0Vly9fRmRkJBYsWID+/fsDAFQqFSZMmIAPPvgAW7duxYkTJ/Dqq6/Cx8cH/fr1M23hS8rODti0CZg6Vd6fM0c2qT14YHBYrVrA2LGyRujuXWDzZuA//wG8vYF794DevWU/bTalERERFV+5HnZ///59TJs2DZGRkUhMTISPjw9efPFFTJ8+HVZWVgDkxIwzZszAypUrkZycjGeffRbLli1D3bp1i/06ZTrsvji+/RYYMQLIygJatAC2bgWqVi3yKZmZQHg48NVX8v5//gMsXSprkYiIiCojY35/l+tAVFbKXSACZBNa//7AnTuy+uedd4BXXwWKKJ8QwMKFsr+RViub2jZuBNzcyqzUREREZUYx8xAp2rPPAocOAY0aAbduyXayqlWB0aOBU6cKfIpKBUycKCuUHBxk5+s2bTiPERER0eMwEJVnfn5yuY8lS+QQs9RUYPlyoHFjWf2zYYNsVntEr15y9H6NGsD58zIU7dhR9sUnIiKqKBiIyjs7O9k56NQpOQZ/4EC5YGx0tOx0XbOmnOTx5k2DpzVuLCuY2rYFNBqgRw9g2TLTvAUiIqLyjoGoolCp5BxFGzcCly8D06YBnp4yCM2cKauDXnpJDjX7f7p5jF55BcjJkblq7FguGktERPQoBqKKqFo1uQrs1avA2rWyv1F2trzdrZvB+h5qtRy09v9zWWLJEtmkdvu2aYpORERUHjEQVWRWVkBoKLB3L/DXX4C7O/D330BIiGwn+38qlZze6KefcheNbdoUyDOfJRERkaIxEFUWgYHAzp1yjP2hQ3Lq6pQUg0P69wdiYmT/7Fu3gK5dZVAqoF82ERGRojAQVSZNm8pQ5Ooqa4x69ADu3zc4pEkTWYn0xhty3qKPPgLatZNLghARESkVA1Fl06yZHGPv7CzH3hcQimxtgS++kCuFuLjkri37/femKDAREZHpMRBVRi1ayJoiZ2dg/37Zizo1Nd9hAwYAx44Bzz0nH371VeDll/O1tBEREVV6DESVVUCA7D3t5CQ7XT//fL6FYgHA11cOzX//fTm90erVQPPmcj5IIiIipWAgqsxatQK2b5frn0VHA717A2lp+Q4zNwfeew/Ys0dOZ3TxohzJ/847Mhix0zUREVV2XNwV5XRxV2OKiZHzE6WmAl26AD//DNjYFHhocjIwahSwbl3uPltbOeP1c8/JLTAQsLYum6ITEREVhqvdG1mlD0SA7EvUvbsMRcHBclIiB4cCDxUC+PFHOc/j3r1AUpLh41ZWMhTpAlK7dnKFESIiorLEQGRkighEALBvnwxFDx4AjRoBW7YAtWsX+RStFjh9Wjan7dkjW97i4w2PcXUFPvwQGDFCNr8RERGVBQYiI1NMIAJkp6B+/WSqcXWVVUFduhT76ULIOYt0AenPP4Fr1+RjzZvLpUHati2dohMREeVlzO9vdqpWmsBAOTNj69ayLSwkBFi0SCadYlCpgDp1gNdfB1atkh2wP/9cjvD/5x/ZfDZ0aP5aJCIiovKMgUiJqlaVbV+vvgrk5AATJgDDhwMZGSU+lYUFMGYM8O+/wH/+IwPTd98BdesCCxYUb4Sartbpm2/kDNoffgjcuVPyt0VERPSk2GQGhTWZ5SUEsHAhMGWK7CzUpo3sbO3t/cSnjI2VAenQIXm/QQNZg5S3VU4IGaCio3O3GzcMz2NjAwwbBkyaJGukiIiIHsU+REam2ECks2MHMGQIcO8e4OMDREbKJrUnpNXK5rSpU4Hbt+W+F14AOnbM7ZidkGD4HEtL+ZJBQbJf0pEjcr9KJRelnTJFPkZERKTDQGRkig9EAHD+PNCnD3DmDKBWAytXyia1p5CcDMyYITtaa7WGj6nVskKqQwe5tWkj5zsCZA3S7t3AJ58A27blPqddOxmM+vQBzNjYS0SkeAxERsZA9P9SUoBXXgG2bpX3R4+WCcTP76lOe/w4MGuWPP1zz8kA1Lp18SZ3PHUK+PRT4Icfcvsj+fsDkycDPXsCXl6ydomIiJSHgcjIGIjy0GqBmTPl4mY6HTvKDj0vvGCyGRhv3ZJ9kZYvlzVPebm7y25P3t4yIOlu67YGDeQxRERUuTAQGRkDUQF++w347DNg587cIfn29sCgQTIctW8vO/iUsfv35Wi0FStkK192dvGe5+sLtGgh50pq0UJuPj4meQtERGQkDERGxkBUhKtXge+/l72kz5/P3V+rlgxGr74qV4Q1Aa0WuHtX1h7l3eLjc29fvw5cvlzw8z08ckNS8+YyNHl4yM3RkWGJiKi8YyAyMgaiYhBCroe2ahWwfr1cE02nZ0/g44/lciDlUEoKcPSonDjyyBH58/RpOQVTYSwtc8NR3s3TU4anoCA5GSUREZkOA5GRMRCV0IMHcr6iVavkGHlALmI2cqTsPe3mZtLiFcfDh8CJE7kh6cQJWbN0+7Zh1itKo0ZymRLd5u/PWiUiorLEQGRkDERP4dw54K235NxFAODiIkPRyJEVdvjXw4cyGBW0Xb8ul4M7dy7/89zdZc1R27byZ716skaJIYmIqHQwEBkZA5ER/PmnXALkxAl5v2FD2Sm7WzeTFqu03L4NxMTIVsQDB+QM3QWtfGJrK7tb1aoF1K5teLtmTTkfU0YGcPOmDFvXr8tZux+9nZws+zU5OeVuzs75b7u6AtWqydVZqlaV5yciqqwYiIyMgchIsrOBr74C3ntP9nYGgN695URC/v6mLVspy8yUzW8HDsiQ9PffwLVr+SekzEulkiHm0WkEjMndXQYkXUjS3fbzk32hnubX/do1YO9e+Z7NzOTEme3by9F7RERlgYHIyBiIjOzePWD2bDlFdXa2bDobN05O9GhvLxcqs7ausE1qxZWZCVy5Aly8KBevvXgx9/aFC7Irlo5anT+05L3t4iKnHNBocrfkZMP7Go2sudLVKqWnP76MdesCAQFAy5byZ4sWgIND/uOEAM6elQFIt125UvA5a9WSwUi3mbJvVXq6DG41a1b6XzciRWIgMjIGolJy9qxcnfW33wp+3NxcBiNdQNLdDgyUz2vQoGzLW4aEAO7cARITZT8jNzfjhgYhZC4tqBnu+nV5aa5ezf88lUr2fdKFJCFk+Nm3L3ddOh1zc1nL1L69HLG3dy9w7Fj+WjFPT+DZZ+VxjRrJ96rbbG2N874zMuSCwadOGW7nz8vy+PgA4eHAf//7dH3+c3JkmK1a1WRzlBJRHgxERsZAVMq2bQPeeUd+CxfU0aYwffsC//uf7KVMRnf7NnD4sNz+/lv+vHat8OOtreWac7qan6AgWeGXl0Yj+1bt2SMD0qFDsqasMGq1YUDSbdbWMigVtWm1ssZNF3wKm0bBwiJ3Ak8bG7k6zYQJxc/bWq1sBl2/Hti4US5MrFIBdeoATZrI7Zln5E8/v8q1zt69e7L128ZGbra28ppVpIECJ0/K/5M9eCADvu4bT3c776ZSyf54Bf1Ourrmr2XUaoGkJDlCNe+WkCB/ZmcDw4cDXbpUrM+sImEgMjIGojKk1cpQlJ4uh3Pl/ZmeLv91+eYbYPPm3Oc8+6wcydazZ+X6timHEhMNA5IQuX2DAgIAK6uSnS89XXY419UyXb0qv2Dv3s1dm85YnJxkDVTerWFD+WW2YYPs43/kSO7xISHAxImy3/+jX1ZarRxNuH69fO7Nm7mPWVoWXnY7OxmOnnlG1rTl5ABpaXJ7+NDwp+62hYX8fENCZOWohUXJ3rcQctTjb78Bv/8uax6DguSagc89J+fPKq7sbOCvv4A//gC2b5fX7tFvCJXKMCDpftatK7sM9uwpw4MpnTsnr926dTIwG4ujo/x9cnDIreEtzmz5HTsCH37I/9uVBgYiI2MgKofOnpXL3X/3Xe63T6NGwJtvAi++WPJvZipXhJDzPenC0Z07ubfv3pW1SkX9T173mK+vDD2NGj1+KRYhZCj77DOZt3XnaNAAGD9e1hydOiW/SH/80bC2zMkJ6NcPGDIECA6Wuf3ECblwse7nqVMlqwAtiKOjrE3o1k0GpMLWVU5NBXbtkgHot9+AS5cKP2fDhjIc6TYvL8PHL12S4eePP4CoKDmRaV729jLYFneZHHNz+X+Yvn2BPn3kiMri0mjk5xgXJ8OlblRmcQLWlSvyuq1bZxh8razk5+nrm1u7CBRe65icnP/3Mjk5fzDMy91dNg17eRluly8DX36ZW0vaowfwwQeyr15x3b0rf19//FH+s+jgUPhoU919b2/5HxglfJ0xEBkZA1E5dvMmsHChXLzs/n25r1o1+V/755+Xf/kF9QImKsLFi3Kx4K+/zv21ytu0Bsgg0LevDEHduj1+CoPsbFkzoQtIFy/KL+O8tSgF/dRogB075JaUZHhOf3/52t26yRVydu6UAWjvXsOmSEtLWRvUvbv889i3D4iOls1Fj6pbVwYjKysZgh6dU8vVFejaVQaybt1kfylA/r/k4cP8NVxpaTKgHTgAbN2aO/OGTsOGMhj16SNrwMzMZOg6dUrOGJ+3z9eNGwV/ts7OhlNX6H56eckQt26dbKrVMTeX7yE0VF7Dp51VPicnt/nw7l35O+PuLl+/SpWiO+xfvSrXyo6IyG3WHThQTtdW2OT+SUkyBG3YIK95ccNoXioVUL++/Mxbt5ZbkyamGVwghFxK6d9/ZR88oPBQZ21dsnMzEBkZA1EFoNHIULRwoWycz8vOLndpex8fw6XufXzkvwQMTVSAlBTZQrt4sawpsbWVzT6DB8v/zdvYlF1ZcnJkzYautiYmpugvQj8/Wcbu3YFOnfL35wJkDcfevTIcRUfLTu+P/otvYSGb2HQBqEULGSie1KVLMhht3SpfM2/fripVZBC7fr3w51erJmvt0tLkl+ejf+6FUalk0AsNlYHD3f3J30NpOH8emDkTWLMmt79SWJjcV7u2DFxbtsiaoB07DK99s2ZyXe2OHWVt3aMjTPPeT06WYbygNRytreVAiNatZVBq1kyOwDTG77luoMi//8qQfe5c7u3z5w1H1RbFyio3ILVsKT+vojAQGRkDUQWSkSEXm122TP61FeevzN4eePllYNQo+V8kokfk5Mjailq1ys/osZQU2Sy2fbvcbt2SX/i6EPQk0xncuyc7iEdHyz+lLl1kmCqtf/bu3ZPNelu2yJqtvM1xPj4F9/lycjI8x4MH8stdN3VF3iksrl2T/bVefBF44YWKMQfWqVPA9Oly9SNAhs82beQAhLx905o0kcF80CBZq1dSCQmyD9jBg/Lchw4VPueZt7f83ffzk5vudq1auU3Rd+7kjlYtaPLY69eLXvbIzEyes04dGcIfDXOPNtUCsm/dnj1Fv08GIiNjIKrA7t/Pv9x93mXvz50znDAnKEguKzJoUNn+959I4TIzZYdtc3MZfFxcTF0i0zp8GJg2zXBWkmeekf80DRokm7uMSauVNTWHDsmQdPAgcObM49dutLSUgaio0aI6KpXsq+XvL0Ocv3/u5udXdNfPnJz8c62p1bI2qygMREbGQFSJCQHs3g0sXy7XW9PVQ7u6AsOGyYlpnuS/X0RERhATI2e579Sp7KdeE0L2ibp0Sda4XbpkePvKFcOmO0/PgieO1d329S37/2cyEBkZA5FCxMfLDiNffGE4K2GXLrLWqFcv1hoREf2/nBzZFCaEbDorj4N7GYiMjIFIYXJyZMeGFSuAX3/N7WVqbS3HC3ftKsdWN2vGeY+IiMoxBiIjYyBSsCtX5EQh336bf+iLu7usPQoOliGpRg3TlJGIiArEQGRkDESkX71UNyHM7t35exv6+8uA1KyZnIa4fn3ZqM45+YmITIKByMgYiCifrCw5DGPHDjkz2sGDBS+W5egow5EuIOlu+/uXfIYxIiIqEQYiI2MgosfSaGSt0Z49siYpLk4Ow3h0aXcdMzM5hjYwUE4yEhgoAxP7JBERGQ0DkZExENETyciQE3voAlJcXO5tjSb/8U5OQKtWuQEpMLBkK28SEZEBBiIjYyAioxJCTt966JCcie7gQbl8fFpa/mN9fOTMZzk5srZJ9zPv7ZwceUytWnKa10c3Ly/2YyIiRWIgMjIGIip12dlypU1dQNJNE2sMdnZyMaQ6deTMbm3byu1pV7QkIirnGIiMjIGITCI5Wa7HBsj1DMzMCv/58KGcPla3UqJuu3Kl4H5MKpVcDKl9e7k9+2zFWOiJiKgEGIiMjIGIKqzMTLnypS4gHT0qlzc/fz7/sbVq5Qakdu3kkiXs5E1EFRgDkZExEFGlc+sWsG+fDEd79wLHjuXOyK3j4AC0aAEEBAAtW8qfdeoUHZLS0mTT3/HjuduJE3L6gQED5JLjgYEMWkRUJhiIjIyBiCo9jUauIqkLSH//LZvhHuXoKEOSLiDZ2BiGn3Pn8gerR1WtCgwcKLd27WSzHxFRKWAgMjIGIlKc7GzZqfvwYRmODh+WzW3p6Y9/bpUqQNOmso9SkyZyvqUrV4CNG4GtW4H793OP9fSUNUeDBsmmOgsL45Q/K0s2C9rYyABmaWmc8xJRhcJAZGQMRESQIePMmdyA9Pffso+SLvjoNk/Pws+Rni5n9t64EdiyRXYc13F3Bzp2lH2X8m5ubkWXKz1dNssdOSK3f/6RtVUZGfJxlUpOPeDrC1SvLn/m3apX59QERJUUA5GRMRARlYLMTODPP2U4iowEkpIKPs7V1TAg1akDJCTkBqDTpwteNsXeXr5GZubjy1K7NtC3r9zYjEdUaTAQGRkDEVEpy8qSfZeOH5czef/7r9yuXy/e893dZd+mvJufn3zs9m3g2rXc7epVw/s3bxpOTeDmBjz/PNCvH9C1q5zHqShCAImJchbys2flOV1dAW9vOZWBj4+8bW//RB8NET05BiIjYyAiMpEHD2RfoH//lR22dT9dXWWnbl34qVr1yZu8UlOB7dtlE94vvwD37uU+Zm0tQ1HfvkD37kBKSm7w0S3FcvZswUuxPMrBwTAg+fjIpjpPT/lTd9vNreKMwrt3T753b29ArTZ1aYjyYSAyMgYiIoXIzpbTEWzZIrdLl4r3PDMzoGZNuUBvjRqyb9TNm3J6gxs3ZLArLnNzGYx0QcnbW9Z21aolNz8/2XHdFH2ehJBB8Oef5bZ/f27tmptb/lox3W0fH9m5/nG1bURGxkBkZAxERAokhJxTSReO/v5b1vLUrw/Uq2f4s04dWZtUmPv3cwPSzZu5W0KC3OLj5Xb3bvHKZmubG5B0IcnTU3Ykf/hQzgf18KHhbd1PKyvA31+WvV49ebuooJKVJYPP1q0yBD06qaeVVfH6aXl4AF99BfTpU7z3SGQEDERGxkBEREhLk8P4S7NmJitL9keKj88NSjduyJqqixfldv364+d6Kqlq1XIDkm67d0+GoN9+MxwNaGkJdOokg83zz8tRevfuGQa9vMHv1i3gwgX5vgBgxAhgwYLy0adKCGDtWuDzz2UNV+vWQKtWcnN3N3XpyAgYiIyMgYiIyo2MDNkxXBeQdNudOzKw6TZb24Jvp6XJvlhxcXIrTq2UrqN5795At26ypqykZX7vPeDTT2UIqVMH+OEHOWu5qVy8CIwaBfzxR8GP+/nJYKQLSS1alI8QRyXCQGRkDEREVGndvWsYkHSbhQXQo4cMQW3aGGcqgl27gFdflbVc5ubA9OnAO+8Yb0LO4sjKkjVUs2bJJkS1Gpg6FXBxAWJj5aZbVDkvMzOgQQMZlPL2jcq7eXgYdohPS5Ph9epVOTnplSuGtzMzgTFjgMmT2Sm9lCgqEN24cQNvvfUWfvvtN6SlpaFOnTqIiIhAy5YtAQBCCMyYMQNffvklkpOT0a5dOyxfvhz+/v7Ffg0GIiIiI7l3Dxg9Gli3Tt5v00bWFtWuXfqvffAg8MYbcnoHQDb9rVgh57fKKzlZ9hmLjQUOHZI/b9x4/PktLGRnchcX2VR4+3bxylW7NrBwIdCrFycINTLFBKJ79+6hefPm6NSpE0aNGgUPDw+cO3cOtWvXRu3//+P66KOPMHfuXHz77bfw8/PDtGnTcOLECZw+fRrWRXWCzIOBiIjIyNaskcFIo5GduhctAl5/vXiBQIiSBYeUFODdd4GlS+Vz3dxk892rrxb/PLduyVnQr1837C+l2xITC+7b5eAgRx5Wry5/6rbq1WXfqrfekucGZI3cwoX5Axo9McUEoqlTp2L//v3Yu3dvgY8LIeDj44PJkydjypQpAACNRgNPT0+sWrUKoaGhxXodBiIiolJw9aoMJdHR8n6/fjIk3b0ra1fybnfu5N6+e1fWwvj7y/5Ij26urrlBJzISGDs2t4bnlVdkGPLwMO57ycqSHeFv3pSzrnt5yeDj7Fx06Lp/H/jwQ9mMl5UlO61PnCj7XBW3r5ZWK+fnunlTvq8qVWTo44zryglEDRs2REhICK5fv47o6GhUrVoVo0ePxogRIwAAFy9eRO3atfHPP/+gWbNm+ud16NABzZo1w6JFiwo8b0ZGBjJ06yBBfqC+vr4MRERExpaTI8PAu+/KQGAMzs4yGKnVcsoAQDZLrVgBBAcb5zWM7d9/gQkT5Kg+QDa9ffwxEBaWP1DdvSub//JueUcCAvI57u4yHD26ubjI4GVhIX8WtlWrVjZNmaVIMYFI1+Q1adIkDBo0CLGxsRg/fjxWrFiBoUOH4sCBA2jXrh1u3rwJb29v/fMGDx4MlUqF9evXF3jemTNnYtasWfn2MxAREZWSo0eBSZNk05OHh/wy9/Aw3HT73NxkjdH58/m3R5d7sbAA/vc/WeNiY2OSt1Yiv/wig9GFC/J+27bA22/LqRd04efRuaAAOQ9WjRoyLN29a7ypGfz95QjDXr2A9u3lvFMViGICkZWVFVq2bIkDBw7o940bNw6xsbGIiYl54kDEGiIiogoqLU0OqT9/XjaTde4sR4dVJBkZstbsgw/k+ylIvXpy2oLAQNkx/ZlnZK0OIGdcv3tXhsu8W0KC/KnRyNq4orbMTBnC8tbaOTjIaRd69QJ69pSTgZZzxgxEZTgWsuS8vb3RsGFDg30NGjTApk2bAABeXl4AgISEBINAlJCQYNCE9ii1Wg01h0ASEVU8trZA48Zyq6jUalkr9Mor8ueePUCjRrnhp3Vr2exVGAuL3OVfnkZKCrBjB/Drr8C2bTJQbdokN0DOz9Srl/ysc3JkeMrOzr/p9ltby+ZMFxfDn87OMmyV8xF25ToQtWvXDnFxcQb7/v33X9SoUQMA4OfnBy8vL0RFRekDUEpKCg4ePIhRo0aVdXGJiIiKr1o14PvvTff6jo7AwIFy02qBw4dlOPr119xpCWJjjfNaZma5IcnKKjccqVSFb02bAhERxnn9YijXgWjixIlo27Yt5syZg8GDB+PQoUNYuXIlVq5cCQBQqVSYMGECPvjgA/j7++uH3fv4+KBfv36mLTwREVFFYWaWu6zJzJlyqoDffsutObKwKHjTdd42NwfS02Xn73v35E/d7cxMGbiSkuRWXMWcOsdYynUfIgD45Zdf8Pbbb+PcuXPw8/PDpEmT9KPMgNyJGVeuXInk5GQ8++yzWLZsGeqWYJ4HDrsnIiIqBULkD0qZmXL/4zZnZyAoqMjTK6ZTdVlhICIiIqp4jPn9bfb4Q4iIiIgqNwYiIiIiUjwGIiIiIlI8BiIiIiJSPAYiIiIiUjwGIiIiIlI8BiIiIiJSPAYiIiIiUjwGIiIiIlI8BiIiIiJSPAYiIiIiUjwGIiIiIlI8BiIiIiJSPAYiIiIiUjwGIiIiIlI8BiIiIiJSvBIHoqysLFhYWODkyZOlUR4iIiKiMlfiQGRpaYnq1asjJyenNMpDREREVOaeqMns3XffxTvvvIOkpCRjl4eIiIiozFk8yZOWLFmC8+fPw8fHBzVq1ICdnZ3B40eOHDFK4YiIiIjKwhMFon79+hm5GERERESmoxJCCFMXwtRSUlLg5OQEjUYDR0dHUxeHiIiIisGY399PVEOkc/jwYZw5cwYA0KhRIzRv3vypCkNERERkCk8UiBITExEaGordu3fD2dkZAJCcnIxOnTph3bp18PDwMGYZiYiIiErVE40yGzt2LO7fv49Tp04hKSkJSUlJOHnyJFJSUjBu3Dhjl5GIiIioVD1RHyInJyfs3LkTrVq1Mth/6NAhdOvWDcnJycYqX5lgHyIiIqKKx5jf309UQ6TVamFpaZlvv6WlJbRa7VMViIiIiKisPVEg6ty5M8aPH4+bN2/q9924cQMTJ05Ely5djFY4IiIiorLwRIFoyZIlSElJQc2aNVG7dm3Url0bfn5+SElJweeff27sMhIRERGVqicaZebr64sjR45g586dOHv2LACgQYMGCA4ONmrhiIiIiMpCiQNRVlYWbGxscPToUXTt2hVdu3YtjXIRERERlRmudk9ERESKx9XuiYiISPG42j0REREpHle7JyIiIsUrcSDKzs6GSqXC66+/jmrVqpVGmYiIiIjKVIn7EFlYWGD+/PnIzs4ujfIQERERlbknnqk6Ojra2GUhIiIiMokn6kPUo0cPTJ06FSdOnEBAQEC+TtV9+vQxSuGIiIiIysITrXZvZlZ4xZJKpapwcxRxtXsiIqKKx5jf309UQ8QV7YmIiKgyKVEfop49e0Kj0ejvz5s3D8nJyfr7d+/eRcOGDY1WOCIiIqKyUKJAtH37dmRkZOjvz5kzx2C26uzsbMTFxRmvdERERERloESB6NHuRk/Q/YiIiIio3HmiYfdERERElUmJApFKpYJKpcq3j4iIiKgiK9EoMyEEhg0bBrVaDQBIT0/HyJEj9fMQ5e1fRERERFRRlCgQDR061OD+yy+/nO+YV1999elKRERERFTGShSIIiIiSqscRERERCbDTtVERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgVKhDNmzcPKpUKEyZM0O9LT09HeHg43NzcYG9vj4EDByIhIcF0hSQiIqIKp8IEotjYWHzxxRdo0qSJwf6JEyfi559/xoYNGxAdHY2bN29iwIABJiolERERVUQVIhClpqYiLCwMX375JVxcXPT7NRoNvv76ayxYsACdO3dGQEAAIiIicODAAfz1118mLDERERFVJBUiEIWHh6NXr14IDg422H/48GFkZWUZ7K9fvz6qV6+OmJiYQs+XkZGBlJQUg42IiIiUy8LUBXicdevW4ciRI4iNjc33WHx8PKysrODs7Gyw39PTE/Hx8YWec+7cuZg1a5axi0pEREQVVLmuIbp27RrGjx+P1atXw9ra2mjnffvtt6HRaPTbtWvXjHZuIiIiqnjKdSA6fPgwEhMT0aJFC1hYWMDCwgLR0dFYvHgxLCws4OnpiczMTCQnJxs8LyEhAV5eXoWeV61Ww9HR0WAjIiIi5SrXTWZdunTBiRMnDPa99tprqF+/Pt566y34+vrC0tISUVFRGDhwIAAgLi4OV69eRVBQkCmKTERERBVQuQ5EDg4OaNy4scE+Ozs7uLm56fcPHz4ckyZNgqurKxwdHTF27FgEBQWhTZs2pigyERERVUDlOhAVx2effQYzMzMMHDgQGRkZCAkJwbJly0xdLCIiIqpAVEIIYepCmFpKSgqcnJyg0WjYn4iIiKiCMOb3d7nuVE1ERERUFhiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxynUgmjt3Llq1agUHBwdUqVIF/fr1Q1xcnMEx6enpCA8Ph5ubG+zt7TFw4EAkJCSYqMRERERUEZXrQBQdHY3w8HD89ddf2LFjB7KystCtWzc8ePBAf8zEiRPx888/Y8OGDYiOjsbNmzcxYMAAE5aaiIiIKhqVEEKYuhDFdfv2bVSpUgXR0dF47rnnoNFo4OHhgTVr1uCFF14AAJw9exYNGjRATEwM2rRpU6zzpqSkwMnJCRqNBo6OjqX5FoiIiMhIjPn9Xa5riB6l0WgAAK6urgCAw4cPIysrC8HBwfpj6tevj+rVqyMmJqbQ82RkZCAlJcVgIyIiIuWqMIFIq9ViwoQJaNeuHRo3bgwAiI+Ph5WVFZydnQ2O9fT0RHx8fKHnmjt3LpycnPSbr69vaRadiIiIyrkKE4jCw8Nx8uRJrFu37qnP9fbbb0Oj0ei3a9euGaGEREREVFFZmLoAxTFmzBj88ssv2LNnD6pVq6bf7+XlhczMTCQnJxvUEiUkJMDLy6vQ86nVaqjV6tIsMhEREVUg5bqGSAiBMWPGIDIyEn/++Sf8/PwMHg8ICIClpSWioqL0++Li4nD16lUEBQWVdXGJiIiogirXNUTh4eFYs2YNtmzZAgcHB32/ICcnJ9jY2MDJyQnDhw/HpEmT4OrqCkdHR4wdOxZBQUHFHmFGREREVK6H3atUqgL3R0REYNiwYQDkxIyTJ0/G2rVrkZGRgZCQECxbtqzIJrNHcdg9ERFRxWPM7+9yHYjKCgMRERFRxaPYeYiIiIiISgMDERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESkeAxEREREpHgMRERERKR4DERERESlepQlES5cuRc2aNWFtbY3AwEAcOnTI1EUiIiKiCqJSBKL169dj0qRJmDFjBo4cOYKmTZsiJCQEiYmJpi4aERERVQCVIhAtWLAAI0aMwGuvvYaGDRtixYoVsLW1xTfffGPqohEREVEFUOEDUWZmJg4fPozg4GD9PjMzMwQHByMmJsaEJSMiIqKKwsLUBXhad+7cQU5ODjw9PQ32e3p64uzZswU+JyMjAxkZGfr7Go0GAJCSklJ6BSUiIiKj0n1vCyGe+lwVPhA9iblz52LWrFn59vv6+pqgNERERPQ07t69Cycnp6c6R4UPRO7u7jA3N0dCQoLB/oSEBHh5eRX4nLfffhuTJk3S309OTkaNGjVw9erVp/5A6emkpKTA19cX165dg6Ojo6mLo2i8FuUHr0X5wWtRvmg0GlSvXh2urq5Pfa4KH4isrKwQEBCAqKgo9OvXDwCg1WoRFRWFMWPGFPgctVoNtVqdb7+TkxN/wcsJR0dHXotygtei/OC1KD94LcoXM7On7xJd4QMRAEyaNAlDhw5Fy5Yt0bp1ayxcuBAPHjzAa6+9ZuqiERERUQVQKQLRkCFDcPv2bUyfPh3x8fFo1qwZfv/993wdrYmIiIgKUikCEQCMGTOm0Cayx1Gr1ZgxY0aBzWhUtngtyg9ei/KD16L84LUoX4x5PVTCGGPViIiIiCqwCj8xIxEREdHTYiAiIiIixWMgIiIiIsVjICIiIiLFU3wgWrp0KWrWrAlra2sEBgbi0KFDpi6SIuzZswe9e/eGj48PVCoVNm/ebPC4EALTp0+Ht7c3bGxsEBwcjHPnzpmmsJXY3Llz0apVKzg4OKBKlSro168f4uLiDI5JT09HeHg43NzcYG9vj4EDB+abGZ6MY/ny5WjSpIl+0r+goCD89ttv+sd5LUxj3rx5UKlUmDBhgn4fr0XZmTlzJlQqlcFWv359/ePGuhaKDkTr16/HpEmTMGPGDBw5cgRNmzZFSEgIEhMTTV20Su/Bgwdo2rQpli5dWuDjH3/8MRYvXowVK1bg4MGDsLOzQ0hICNLT08u4pJVbdHQ0wsPD8ddff2HHjh3IyspCt27d8ODBA/0xEydOxM8//4wNGzYgOjoaN2/exIABA0xY6sqrWrVqmDdvHg4fPoy///4bnTt3Rt++fXHq1CkAvBamEBsbiy+++AJNmjQx2M9rUbYaNWqEW7du6bd9+/bpHzPatRAK1rp1axEeHq6/n5OTI3x8fMTcuXNNWCrlASAiIyP197VarfDy8hLz58/X70tOThZqtVqsXbvWBCVUjsTERAFAREdHCyHk525paSk2bNigP+bMmTMCgIiJiTFVMRXFxcVFfPXVV7wWJnD//n3h7+8vduzYITp06CDGjx8vhODfRVmbMWOGaNq0aYGPGfNaKLaGKDMzE4cPH0ZwcLB+n5mZGYKDgxETE2PCktGlS5cQHx9vcG2cnJwQGBjIa1PKNBoNAOgXSjx8+DCysrIMrkX9+vVRvXp1XotSlpOTg3Xr1uHBgwcICgritTCB8PBw9OrVy+AzB/h3YQrnzp2Dj48PatWqhbCwMFy9ehWAca9FpZmpuqTu3LmDnJycfMt7eHp64uzZsyYqFQFAfHw8ABR4bXSPkfFptVpMmDAB7dq1Q+PGjQHIa2FlZQVnZ2eDY3ktSs+JEycQFBSE9PR02NvbIzIyEg0bNsTRo0d5LcrQunXrcOTIEcTGxuZ7jH8XZSswMBCrVq1CvXr1cOvWLcyaNQvt27fHyZMnjXotFBuIiMhQeHg4Tp48adA2T2WvXr16OHr0KDQaDTZu3IihQ4ciOjra1MVSlGvXrmH8+PHYsWMHrK2tTV0cxevRo4f+dpMmTRAYGIgaNWrgxx9/hI2NjdFeR7FNZu7u7jA3N8/XEz0hIQFeXl4mKhUB0H/+vDZlZ8yYMfjll1+wa9cuVKtWTb/fy8sLmZmZSE5ONjie16L0WFlZoU6dOggICMDcuXPRtGlTLFq0iNeiDB0+fBiJiYlo0aIFLCwsYGFhgejoaCxevBgWFhbw9PTktTAhZ2dn1K1bF+fPnzfq34ViA5GVlRUCAgIQFRWl36fVahEVFYWgoCATloz8/Pzg5eVlcG1SUlJw8OBBXhsjE0JgzJgxiIyMxJ9//gk/Pz+DxwMCAmBpaWlwLeLi4nD16lVeizKi1WqRkZHBa1GGunTpghMnTuDo0aP6rWXLlggLC9Pf5rUwndTUVFy4cAHe3t7G/bt4io7fFd66deuEWq0Wq1atEqdPnxZvvPGGcHZ2FvHx8aYuWqV3//598c8//4h//vlHABALFiwQ//zzj7hy5YoQQoh58+YJZ2dnsWXLFnH8+HHRt29f4efnJx4+fGjiklcuo0aNEk5OTmL37t3i1q1b+i0tLU1/zMiRI0X16tXFn3/+Kf7++28RFBQkgoKCTFjqymvq1KkiOjpaXLp0SRw/flxMnTpVqFQq8ccffwgheC1MKe8oMyF4LcrS5MmTxe7du8WlS5fE/v37RXBwsHB3dxeJiYlCCONdC0UHIiGE+Pzzz0X16tWFlZWVaN26tfjrr79MXSRF2LVrlwCQbxs6dKgQQg69nzZtmvD09BRqtVp06dJFxMXFmbbQlVBB1wCAiIiI0B/z8OFDMXr0aOHi4iJsbW1F//79xa1bt0xX6Ers9ddfFzVq1BBWVlbCw8NDdOnSRR+GhOC1MKVHAxGvRdkZMmSI8Pb2FlZWVqJq1apiyJAh4vz58/rHjXUtVEIIYYQaLCIiIqIKS7F9iIiIiIh0GIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiIhI8RiIiIiISPEYiIiIiEjxGIiIiAqgUqmwefNmUxeDiMoIAxERlTvDhg2DSqXKt3Xv3t3URSOiSsrC1AUgIipI9+7dERERYbBPrVabqDREVNmxhoiIyiW1Wg0vLy+DzcXFBYBszlq+fDl69OgBGxsb1KpVCxs3bjR4/okTJ9C5c2fY2NjAzc0Nb7zxBlJTUw2O+eabb9CoUSOo1Wp4e3tjzJgxBo/fuXMH/fv3h62tLfz9/bF169bSfdNEZDIMRERUIU2bNg0DBw7EsWPHEBYWhtDQUJw5cwYA8ODBA4SEhMDFxQWxsbHYsGEDdu7caRB4li9fjvDwcLzxxhs4ceIEtm7dijp16hi8xqxZszB48GAcP34cPXv2RFhYGJKSksr0fRJRGTHeerRERMYxdOhQYW5uLuzs7Ay2Dz/8UAghBAAxcuRIg+cEBgaKUaNGCSGEWLlypXBxcRGpqan6x3/99VdhZmYm4uPjhRBC+Pj4iHfffbfQMgAQ7733nv5+amqqACB+++03o71PIio/2IeIiMqlTp06Yfny5Qb7XF1d9beDgoIMHgsKCsLRo0cBAGfOnEHTpk1hZ2enf7xdu3bQarWIi4uDSqXCzZs30aVLlyLL0KRJE/1tOzs7ODo6IjEx8UnfEhGVYwxERFQu2dnZ5WvCMhYbG5tiHWdpaWlwX6VSQavVlkaRiMjE2IeIiCqkv/76K9/9Bg0aAAAaNGiAY8eO4cGDB/rH9+/fDzMzM9SrVw8ODg6oWbMmoqKiyrTMRFR+sYaIiMqljIwMxMfHG+yzsLCAu7s7AGDDhg1o2bIlnn32WaxevRqHDh3C119/DQAICwvDjBkzMHToUMycORO3b9/G2LFj8corr8DT0xMAMHPmTIwcORJVqlRBjx49cP/+fezfvx9jx44t2zdKROUCAxERlUu///47vL29DfbVq1cPZ8+eBSBHgK1btw6jR4+Gt7c31q5di4YNGwIAbG1tsX37dowfPx6tWrWCra0tBg4ciAULFujPNXToUKSnp+Ozzz7DlClT4O7ujhdeeKHs3iARlSsqIYQwdSGIiEpCpVIhMjIS/fr1M3VRiKiSYB8iIiIiUjwGIiIiIlI89iEiogqHLf1EZGysISIiIiLFYyAiIiIixWMgIiIiIsVjICIiIiLFYyAiIiIixWMgIiIiIsVjICIiIiLFYyAiIiIixWMgIiIiIsX7P7EPVq2wMSnrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(errors_train,'r-',label='train')\n",
        "ax.plot(errors_val,'b-',label='validation')\n",
        "ax.set_ylim(0,100); ax.set_xlim(0,epochs)\n",
        "ax.set_xlabel('Epoch'); ax.set_ylabel('Error')\n",
        "ax.set_title('Part I: Validation Result %3.2f'%(errors_val[-1]))\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save best performing model to be deployed together with the hyperparameters\n",
        "PATH = f\"trained_model_cifar100_{groupid}\"\n",
        "torch.save(trained_model, PATH)"
      ],
      "metadata": {
        "id": "WgDrhTmS1J1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfMGFs4YoV_I"
      },
      "source": [
        "# Test Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEZluoBZgy2c"
      },
      "outputs": [],
      "source": [
        "# Run on test data\n",
        "def test(model):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in testloader:\n",
        "      output = model(data)\n",
        "      test_loss += loss_fn(output, target).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(testloader.dataset)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(testloader.dataset),\n",
        "    100. * correct / len(testloader.dataset)))\n",
        "  return test_loss, 100. * correct / len(testloader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp_IwAiDZds9",
        "outputId": "02106209-494e-42cc-940b-9eb03fc458f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.0426, Accuracy: 3195/10000 (32%)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.04264784121513367, tensor(31.9500))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "test(trained_model)\n",
        "\n",
        "# Test accuracy of our first developed model is 32%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCMRstw11yKQ"
      },
      "source": [
        "##### Experiment with Hyperparameters and Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMIRfxUr2BT3"
      },
      "outputs": [],
      "source": [
        "## set model hyperparameters\n",
        "d_in = 3 # rgb - 3 channels\n",
        "d_out = 100\n",
        "\n",
        "learning_rate = [0.01, 0.001]\n",
        "batch_sizes = [64, 128]\n",
        "optimizers = ['SGD', 'Adam']\n",
        "\n",
        "# Creating optimizer function\n",
        "def get_optimizer(model, optimizer_name, lr):\n",
        "  if optimizer_name == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.5)\n",
        "    return optimizer\n",
        "  elif optimizer_name == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F47Rc9P6ok-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cab419d-2a3e-4d5c-b5a3-4cc33760c3eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch     0, train loss 3464.922852, train accuracy 4.06, val loss 695.475918, val error 96.16\n",
            "Epoch     1, train loss 3331.320698, train accuracy 6.66, val loss 668.915283, val error 93.86\n",
            "Epoch     2, train loss 3237.485664, train accuracy 9.42, val loss 650.735198, val error 90.78\n",
            "Epoch     3, train loss 3153.571194, train accuracy 9.89, val loss 634.531567, val error 90.05\n",
            "Epoch     4, train loss 2988.063787, train accuracy 12.30, val loss 603.359018, val error 87.66\n",
            "Epoch     5, train loss 2898.661586, train accuracy 14.84, val loss 587.536778, val error 85.68\n",
            "Epoch     6, train loss 2812.044463, train accuracy 16.58, val loss 572.297544, val error 84.22\n",
            "Epoch     7, train loss 2743.023578, train accuracy 18.45, val loss 559.735398, val error 82.79\n",
            "Epoch     8, train loss 2711.748390, train accuracy 18.91, val loss 554.017847, val error 81.89\n",
            "Epoch     9, train loss 2621.665107, train accuracy 20.20, val loss 537.268184, val error 80.55\n",
            "Epoch    10, train loss 2566.568774, train accuracy 22.46, val loss 527.039522, val error 79.10\n",
            "Epoch    11, train loss 2521.642445, train accuracy 23.26, val loss 519.915300, val error 78.18\n",
            "Epoch    12, train loss 2486.559678, train accuracy 23.66, val loss 512.895519, val error 77.90\n",
            "Epoch    13, train loss 2450.828268, train accuracy 24.74, val loss 507.448001, val error 77.47\n",
            "Epoch    14, train loss 2425.006645, train accuracy 25.66, val loss 503.313506, val error 76.77\n",
            "Epoch    15, train loss 2398.740896, train accuracy 26.50, val loss 498.764780, val error 75.98\n",
            "Epoch    16, train loss 2331.293349, train accuracy 28.98, val loss 487.234898, val error 74.44\n",
            "Epoch    17, train loss 2286.001262, train accuracy 29.38, val loss 479.384832, val error 73.88\n",
            "Epoch    18, train loss 2275.413060, train accuracy 29.11, val loss 478.809671, val error 74.06\n",
            "Epoch    19, train loss 2221.811267, train accuracy 31.50, val loss 468.772986, val error 72.77\n",
            "Epoch    20, train loss 2191.015615, train accuracy 32.06, val loss 464.697680, val error 72.14\n",
            "Epoch    21, train loss 2170.432274, train accuracy 31.95, val loss 462.406579, val error 72.15\n",
            "Epoch    22, train loss 2138.256048, train accuracy 32.22, val loss 457.591124, val error 71.98\n",
            "Epoch    23, train loss 2137.710284, train accuracy 32.98, val loss 457.053959, val error 71.46\n",
            "Epoch    24, train loss 2112.693753, train accuracy 33.82, val loss 452.280190, val error 70.46\n",
            "Epoch    25, train loss 2152.888888, train accuracy 32.06, val loss 461.813262, val error 72.54\n",
            "Epoch    26, train loss 2055.567530, train accuracy 35.67, val loss 444.112401, val error 69.17\n",
            "Epoch    27, train loss 2046.023715, train accuracy 36.29, val loss 442.965097, val error 69.26\n",
            "Epoch    28, train loss 2020.861737, train accuracy 36.63, val loss 439.914954, val error 69.02\n",
            "Epoch    29, train loss 1992.804053, train accuracy 37.27, val loss 435.511399, val error 68.41\n",
            "Epoch    30, train loss 2030.384553, train accuracy 35.73, val loss 441.626442, val error 69.61\n",
            "Epoch    31, train loss 1978.621274, train accuracy 37.76, val loss 435.459562, val error 68.28\n",
            "Epoch    32, train loss 1994.899884, train accuracy 37.28, val loss 438.359824, val error 68.73\n",
            "Epoch    33, train loss 1975.514734, train accuracy 37.60, val loss 435.527443, val error 68.22\n",
            "Epoch    34, train loss 1998.318510, train accuracy 37.04, val loss 440.932867, val error 68.77\n",
            "Epoch    35, train loss 2029.765207, train accuracy 35.69, val loss 446.156516, val error 70.52\n",
            "Epoch    36, train loss 1938.617977, train accuracy 38.28, val loss 431.312293, val error 68.57\n",
            "Epoch    37, train loss 1891.349649, train accuracy 39.79, val loss 423.774778, val error 67.41\n",
            "Epoch    38, train loss 1875.854509, train accuracy 40.41, val loss 421.066487, val error 66.71\n",
            "Epoch    39, train loss 1867.732082, train accuracy 40.26, val loss 421.578870, val error 67.01\n",
            "Epoch    40, train loss 1886.645281, train accuracy 40.23, val loss 424.878770, val error 66.75\n",
            "Epoch    41, train loss 1918.701427, train accuracy 38.95, val loss 430.202258, val error 68.07\n",
            "Epoch    42, train loss 1892.101319, train accuracy 40.13, val loss 425.995324, val error 67.37\n",
            "Epoch    43, train loss 1859.549691, train accuracy 40.92, val loss 421.690680, val error 66.99\n",
            "Epoch    44, train loss 1833.331617, train accuracy 41.16, val loss 417.980685, val error 66.28\n",
            "Epoch    45, train loss 1861.359521, train accuracy 40.76, val loss 423.159489, val error 67.21\n",
            "Epoch    46, train loss 1832.101139, train accuracy 41.42, val loss 419.466637, val error 67.12\n",
            "Epoch    47, train loss 1824.843266, train accuracy 41.80, val loss 419.799986, val error 66.45\n",
            "Epoch    48, train loss 1852.174821, train accuracy 40.50, val loss 425.314723, val error 67.98\n",
            "Epoch    49, train loss 1823.902969, train accuracy 42.45, val loss 419.317524, val error 66.32\n",
            "Batch Size: 64, Optimizer: SGD, Learning Rate: 0.01 -> Accuracy: 42.448, Loss: 1823.9029690027237\n",
            "Epoch     0, train loss 3594.226356, train accuracy 1.37, val loss 721.840647, val error 98.69\n",
            "Epoch     1, train loss 3587.025810, train accuracy 1.56, val loss 720.460788, val error 98.63\n",
            "Epoch     2, train loss 3575.546256, train accuracy 1.89, val loss 718.332641, val error 98.43\n",
            "Epoch     3, train loss 3559.170908, train accuracy 2.30, val loss 715.156688, val error 97.85\n",
            "Epoch     4, train loss 3541.067438, train accuracy 3.01, val loss 711.735292, val error 97.16\n",
            "Epoch     5, train loss 3521.894920, train accuracy 3.48, val loss 708.026300, val error 96.59\n",
            "Epoch     6, train loss 3500.205493, train accuracy 3.92, val loss 703.768183, val error 96.07\n",
            "Epoch     7, train loss 3479.053138, train accuracy 4.10, val loss 699.695221, val error 95.68\n",
            "Epoch     8, train loss 3456.873283, train accuracy 4.45, val loss 695.352242, val error 95.22\n",
            "Epoch     9, train loss 3436.191972, train accuracy 4.80, val loss 691.150791, val error 95.13\n",
            "Epoch    10, train loss 3413.929349, train accuracy 5.29, val loss 686.946815, val error 94.81\n",
            "Epoch    11, train loss 3386.927541, train accuracy 5.79, val loss 681.576227, val error 94.14\n",
            "Epoch    12, train loss 3364.014915, train accuracy 6.30, val loss 677.177795, val error 93.96\n",
            "Epoch    13, train loss 3342.345104, train accuracy 6.77, val loss 673.049230, val error 93.38\n",
            "Epoch    14, train loss 3317.085917, train accuracy 7.18, val loss 668.075610, val error 92.67\n",
            "Epoch    15, train loss 3295.337654, train accuracy 7.78, val loss 663.885718, val error 92.51\n",
            "Epoch    16, train loss 3272.328801, train accuracy 8.37, val loss 659.475340, val error 91.94\n",
            "Epoch    17, train loss 3252.844996, train accuracy 8.70, val loss 655.859461, val error 91.69\n",
            "Epoch    18, train loss 3232.466230, train accuracy 9.15, val loss 651.968959, val error 91.10\n",
            "Epoch    19, train loss 3211.315334, train accuracy 9.49, val loss 647.924815, val error 90.53\n",
            "Epoch    20, train loss 3192.201452, train accuracy 9.97, val loss 644.510331, val error 90.14\n",
            "Epoch    21, train loss 3171.372321, train accuracy 10.34, val loss 640.518480, val error 89.64\n",
            "Epoch    22, train loss 3148.418947, train accuracy 10.46, val loss 636.226340, val error 89.56\n",
            "Epoch    23, train loss 3129.525271, train accuracy 10.94, val loss 632.379059, val error 89.18\n",
            "Epoch    24, train loss 3115.391785, train accuracy 11.32, val loss 629.793622, val error 88.96\n",
            "Epoch    25, train loss 3089.898902, train accuracy 11.70, val loss 624.905887, val error 88.49\n",
            "Epoch    26, train loss 3077.880919, train accuracy 11.95, val loss 622.766763, val error 88.54\n",
            "Epoch    27, train loss 3062.765403, train accuracy 12.32, val loss 619.911178, val error 88.05\n",
            "Epoch    28, train loss 3039.690919, train accuracy 12.69, val loss 615.732956, val error 87.94\n",
            "Epoch    29, train loss 3024.344459, train accuracy 12.89, val loss 612.836763, val error 87.55\n",
            "Epoch    30, train loss 3009.396982, train accuracy 13.38, val loss 609.691120, val error 86.98\n",
            "Epoch    31, train loss 2988.183634, train accuracy 13.69, val loss 605.847352, val error 86.81\n",
            "Epoch    32, train loss 2971.950807, train accuracy 13.94, val loss 602.674739, val error 86.55\n",
            "Epoch    33, train loss 2956.263066, train accuracy 14.48, val loss 599.561985, val error 86.10\n",
            "Epoch    34, train loss 2943.111824, train accuracy 14.73, val loss 597.163842, val error 85.92\n",
            "Epoch    35, train loss 2925.495365, train accuracy 15.17, val loss 594.127136, val error 85.51\n",
            "Epoch    36, train loss 2912.940807, train accuracy 15.56, val loss 591.544086, val error 85.34\n",
            "Epoch    37, train loss 2897.967111, train accuracy 15.53, val loss 588.430922, val error 85.05\n",
            "Epoch    38, train loss 2876.176668, train accuracy 16.02, val loss 584.849400, val error 84.82\n",
            "Epoch    39, train loss 2863.811749, train accuracy 16.23, val loss 582.369319, val error 84.60\n",
            "Epoch    40, train loss 2850.844395, train accuracy 16.50, val loss 579.724716, val error 84.20\n",
            "Epoch    41, train loss 2833.493428, train accuracy 16.79, val loss 576.557394, val error 83.97\n",
            "Epoch    42, train loss 2824.286235, train accuracy 17.08, val loss 574.578451, val error 83.68\n",
            "Epoch    43, train loss 2810.286386, train accuracy 17.46, val loss 572.239159, val error 83.58\n",
            "Epoch    44, train loss 2794.371209, train accuracy 17.79, val loss 569.291931, val error 83.36\n",
            "Epoch    45, train loss 2782.799503, train accuracy 18.17, val loss 567.195191, val error 83.17\n",
            "Epoch    46, train loss 2768.738811, train accuracy 18.36, val loss 564.404323, val error 82.63\n",
            "Epoch    47, train loss 2761.466463, train accuracy 18.63, val loss 563.137987, val error 82.55\n",
            "Epoch    48, train loss 2741.794735, train accuracy 18.83, val loss 559.381613, val error 82.13\n",
            "Epoch    49, train loss 2730.543955, train accuracy 19.25, val loss 557.362588, val error 81.96\n",
            "Batch Size: 64, Optimizer: SGD, Learning Rate: 0.001 -> Accuracy: 42.448, Loss: 1823.9029690027237\n",
            "Epoch     0, train loss 3602.652874, train accuracy 1.00, val loss 723.304718, val error 99.00\n",
            "Epoch     1, train loss 3602.870132, train accuracy 1.00, val loss 723.341991, val error 99.00\n",
            "Epoch     2, train loss 3602.672267, train accuracy 1.00, val loss 723.293457, val error 99.00\n",
            "Epoch     3, train loss 3602.696345, train accuracy 1.00, val loss 723.296443, val error 99.00\n",
            "Epoch     4, train loss 3602.964670, train accuracy 1.00, val loss 723.371830, val error 99.00\n",
            "Epoch     5, train loss 3602.852965, train accuracy 1.00, val loss 723.329356, val error 99.00\n",
            "Epoch     6, train loss 3602.811924, train accuracy 1.00, val loss 723.338793, val error 99.00\n",
            "Epoch     7, train loss 3602.807380, train accuracy 1.00, val loss 723.324871, val error 99.00\n",
            "Epoch     8, train loss 3602.847790, train accuracy 1.00, val loss 723.329301, val error 99.00\n",
            "Epoch     9, train loss 3602.879322, train accuracy 1.00, val loss 723.322587, val error 99.00\n",
            "Epoch    10, train loss 3602.864018, train accuracy 1.00, val loss 723.336339, val error 99.00\n",
            "Epoch    11, train loss 3602.931186, train accuracy 1.00, val loss 723.334606, val error 99.00\n",
            "Epoch    12, train loss 3602.850471, train accuracy 1.00, val loss 723.339881, val error 99.00\n",
            "Epoch    13, train loss 3603.119325, train accuracy 1.00, val loss 723.382061, val error 99.00\n",
            "Epoch    14, train loss 3602.988459, train accuracy 1.00, val loss 723.363599, val error 99.00\n",
            "Epoch    15, train loss 3602.664218, train accuracy 1.00, val loss 723.307646, val error 99.00\n",
            "Epoch    16, train loss 3602.918001, train accuracy 1.00, val loss 723.358891, val error 99.00\n",
            "Epoch    17, train loss 3602.745083, train accuracy 1.00, val loss 723.304887, val error 99.00\n",
            "Epoch    18, train loss 3602.994549, train accuracy 1.00, val loss 723.366990, val error 99.00\n",
            "Epoch    19, train loss 3603.159141, train accuracy 1.00, val loss 723.379343, val error 99.00\n",
            "Epoch    20, train loss 3602.904641, train accuracy 1.00, val loss 723.326790, val error 99.00\n",
            "Epoch    21, train loss 3602.683537, train accuracy 1.00, val loss 723.315218, val error 99.00\n",
            "Epoch    22, train loss 3602.832148, train accuracy 1.00, val loss 723.347494, val error 99.00\n",
            "Epoch    23, train loss 3602.811100, train accuracy 1.00, val loss 723.330824, val error 99.00\n",
            "Epoch    24, train loss 3603.136940, train accuracy 1.00, val loss 723.390879, val error 99.00\n",
            "Epoch    25, train loss 3602.784760, train accuracy 1.00, val loss 723.320320, val error 99.00\n",
            "Epoch    26, train loss 3603.054804, train accuracy 1.00, val loss 723.370892, val error 99.00\n",
            "Epoch    27, train loss 3602.598723, train accuracy 1.00, val loss 723.277770, val error 99.00\n",
            "Epoch    28, train loss 3602.753278, train accuracy 1.00, val loss 723.313416, val error 99.00\n",
            "Epoch    29, train loss 3603.361386, train accuracy 1.00, val loss 723.442245, val error 99.00\n",
            "Epoch    30, train loss 3602.751948, train accuracy 1.00, val loss 723.315317, val error 99.00\n",
            "Epoch    31, train loss 3603.089242, train accuracy 1.00, val loss 723.397957, val error 99.00\n",
            "Epoch    32, train loss 3603.000305, train accuracy 1.00, val loss 723.376564, val error 99.00\n",
            "Epoch    33, train loss 3602.816584, train accuracy 1.00, val loss 723.315455, val error 99.00\n",
            "Epoch    34, train loss 3602.794202, train accuracy 1.00, val loss 723.329421, val error 99.00\n",
            "Epoch    35, train loss 3603.446450, train accuracy 1.00, val loss 723.461583, val error 99.00\n",
            "Epoch    36, train loss 3602.959736, train accuracy 1.00, val loss 723.373262, val error 99.00\n",
            "Epoch    37, train loss 3602.381344, train accuracy 1.00, val loss 723.250675, val error 99.00\n",
            "Epoch    38, train loss 3603.033321, train accuracy 1.00, val loss 723.379252, val error 99.00\n",
            "Epoch    39, train loss 3603.008455, train accuracy 1.00, val loss 723.365762, val error 99.00\n",
            "Epoch    40, train loss 3602.708675, train accuracy 1.00, val loss 723.311899, val error 99.00\n",
            "Epoch    41, train loss 3602.872531, train accuracy 1.00, val loss 723.348030, val error 99.00\n",
            "Epoch    42, train loss 3603.033162, train accuracy 1.00, val loss 723.360628, val error 99.00\n",
            "Epoch    43, train loss 3602.826360, train accuracy 1.00, val loss 723.337881, val error 99.00\n",
            "Epoch    44, train loss 3602.711141, train accuracy 1.00, val loss 723.299450, val error 99.00\n",
            "Epoch    45, train loss 3602.752629, train accuracy 1.00, val loss 723.323617, val error 99.00\n",
            "Epoch    46, train loss 3603.016205, train accuracy 1.00, val loss 723.379567, val error 99.00\n",
            "Epoch    47, train loss 3602.517887, train accuracy 1.00, val loss 723.269126, val error 99.00\n",
            "Epoch    48, train loss 3602.643840, train accuracy 1.00, val loss 723.314962, val error 99.00\n",
            "Epoch    49, train loss 3602.789200, train accuracy 1.00, val loss 723.333023, val error 99.00\n",
            "Batch Size: 64, Optimizer: Adam, Learning Rate: 0.01 -> Accuracy: 42.448, Loss: 1823.9029690027237\n",
            "Epoch     0, train loss 3093.801476, train accuracy 10.20, val loss 624.912953, val error 89.90\n",
            "Epoch     1, train loss 2929.443370, train accuracy 14.99, val loss 593.673113, val error 85.53\n",
            "Epoch     2, train loss 2773.536405, train accuracy 17.34, val loss 564.865749, val error 83.69\n",
            "Epoch     3, train loss 2717.162162, train accuracy 18.83, val loss 554.995584, val error 82.57\n",
            "Epoch     4, train loss 2631.656197, train accuracy 20.84, val loss 538.938188, val error 80.93\n",
            "Epoch     5, train loss 2541.781338, train accuracy 23.59, val loss 522.910460, val error 77.96\n",
            "Epoch     6, train loss 2512.268495, train accuracy 23.67, val loss 518.771013, val error 78.33\n",
            "Epoch     7, train loss 2466.376870, train accuracy 24.82, val loss 511.047356, val error 77.26\n",
            "Epoch     8, train loss 2445.175523, train accuracy 26.10, val loss 506.568861, val error 76.05\n",
            "Epoch     9, train loss 2384.176860, train accuracy 27.03, val loss 497.926994, val error 75.93\n",
            "Epoch    10, train loss 2366.890949, train accuracy 27.36, val loss 495.551749, val error 75.80\n",
            "Epoch    11, train loss 2331.493055, train accuracy 28.14, val loss 489.602072, val error 75.27\n",
            "Epoch    12, train loss 2305.849554, train accuracy 29.25, val loss 485.476888, val error 73.98\n",
            "Epoch    13, train loss 2281.019612, train accuracy 29.51, val loss 481.295178, val error 74.04\n",
            "Epoch    14, train loss 2239.986638, train accuracy 30.40, val loss 475.352754, val error 73.35\n",
            "Epoch    15, train loss 2246.096931, train accuracy 30.29, val loss 475.976728, val error 73.40\n",
            "Epoch    16, train loss 2196.346706, train accuracy 31.36, val loss 468.877752, val error 72.55\n",
            "Epoch    17, train loss 2158.281970, train accuracy 32.23, val loss 462.603273, val error 71.63\n",
            "Epoch    18, train loss 2132.265307, train accuracy 32.56, val loss 458.391991, val error 71.75\n",
            "Epoch    19, train loss 2111.435838, train accuracy 33.70, val loss 455.408102, val error 70.75\n",
            "Epoch    20, train loss 2098.156521, train accuracy 34.10, val loss 453.477710, val error 70.36\n",
            "Epoch    21, train loss 2103.126851, train accuracy 34.31, val loss 454.033472, val error 70.29\n",
            "Epoch    22, train loss 2044.314796, train accuracy 34.75, val loss 446.193846, val error 69.91\n",
            "Epoch    23, train loss 2057.820362, train accuracy 34.54, val loss 449.623779, val error 70.24\n",
            "Epoch    24, train loss 2049.040224, train accuracy 34.85, val loss 449.409506, val error 70.48\n",
            "Epoch    25, train loss 2040.483459, train accuracy 34.98, val loss 447.398314, val error 70.60\n",
            "Epoch    26, train loss 2032.782940, train accuracy 35.35, val loss 447.411165, val error 69.86\n",
            "Epoch    27, train loss 1996.220629, train accuracy 36.63, val loss 441.584644, val error 69.22\n",
            "Epoch    28, train loss 1983.665864, train accuracy 36.47, val loss 440.475672, val error 69.39\n",
            "Epoch    29, train loss 1988.281508, train accuracy 37.34, val loss 440.662732, val error 69.61\n",
            "Epoch    30, train loss 1977.365729, train accuracy 36.75, val loss 439.270751, val error 69.34\n",
            "Epoch    31, train loss 1952.160819, train accuracy 37.60, val loss 436.226132, val error 69.21\n",
            "Epoch    32, train loss 1937.180363, train accuracy 37.63, val loss 433.466933, val error 68.82\n",
            "Epoch    33, train loss 1953.832520, train accuracy 37.75, val loss 437.426453, val error 69.04\n",
            "Epoch    34, train loss 1942.362499, train accuracy 37.99, val loss 435.242029, val error 68.73\n",
            "Epoch    35, train loss 1938.173635, train accuracy 37.84, val loss 436.290313, val error 68.89\n",
            "Epoch    36, train loss 1923.488696, train accuracy 38.18, val loss 434.577541, val error 68.56\n",
            "Epoch    37, train loss 1899.463648, train accuracy 39.05, val loss 429.790189, val error 68.15\n",
            "Epoch    38, train loss 1903.189720, train accuracy 38.65, val loss 431.590790, val error 68.55\n",
            "Epoch    39, train loss 1888.110438, train accuracy 38.96, val loss 431.016850, val error 68.34\n",
            "Epoch    40, train loss 1903.614776, train accuracy 38.96, val loss 434.226743, val error 68.95\n",
            "Epoch    41, train loss 1872.625812, train accuracy 39.68, val loss 428.759353, val error 67.77\n",
            "Epoch    42, train loss 1884.395859, train accuracy 39.41, val loss 432.575588, val error 68.24\n",
            "Epoch    43, train loss 1878.271616, train accuracy 39.71, val loss 431.928999, val error 68.53\n",
            "Epoch    44, train loss 1886.909967, train accuracy 38.94, val loss 432.289438, val error 68.24\n",
            "Epoch    45, train loss 1863.070207, train accuracy 39.54, val loss 429.507185, val error 68.18\n",
            "Epoch    46, train loss 1857.401358, train accuracy 40.65, val loss 429.296536, val error 67.86\n",
            "Epoch    47, train loss 1875.157206, train accuracy 39.33, val loss 432.591499, val error 68.55\n",
            "Epoch    48, train loss 1862.227585, train accuracy 40.50, val loss 431.669540, val error 67.82\n",
            "Epoch    49, train loss 1836.810442, train accuracy 40.61, val loss 425.694008, val error 67.77\n",
            "Batch Size: 64, Optimizer: Adam, Learning Rate: 0.001 -> Accuracy: 42.448, Loss: 1823.9029690027237\n",
            "Epoch     0, train loss 1773.915979, train accuracy 2.57, val loss 358.739531, val error 97.63\n",
            "Epoch     1, train loss 1728.020178, train accuracy 4.31, val loss 349.990051, val error 95.76\n",
            "Epoch     2, train loss 1679.530622, train accuracy 7.86, val loss 340.480814, val error 92.45\n",
            "Epoch     3, train loss 1632.029945, train accuracy 9.27, val loss 331.170930, val error 90.68\n",
            "Epoch     4, train loss 1588.707765, train accuracy 10.48, val loss 322.657729, val error 89.80\n",
            "Epoch     5, train loss 1542.780355, train accuracy 11.78, val loss 313.570204, val error 89.00\n",
            "Epoch     6, train loss 1498.101958, train accuracy 13.43, val loss 305.052132, val error 87.33\n",
            "Epoch     7, train loss 1471.104679, train accuracy 14.66, val loss 299.811111, val error 86.07\n",
            "Epoch     8, train loss 1434.520858, train accuracy 15.91, val loss 293.455673, val error 84.84\n",
            "Epoch     9, train loss 1408.482274, train accuracy 17.20, val loss 288.379200, val error 83.57\n",
            "Epoch    10, train loss 1377.669715, train accuracy 18.33, val loss 282.830741, val error 82.71\n",
            "Epoch    11, train loss 1358.587789, train accuracy 19.20, val loss 279.214549, val error 81.58\n",
            "Epoch    12, train loss 1335.593291, train accuracy 20.60, val loss 274.789696, val error 80.76\n",
            "Epoch    13, train loss 1310.693399, train accuracy 21.41, val loss 270.558591, val error 79.61\n",
            "Epoch    14, train loss 1311.715956, train accuracy 21.65, val loss 270.634259, val error 79.70\n",
            "Epoch    15, train loss 1271.415100, train accuracy 23.44, val loss 262.910148, val error 78.18\n",
            "Epoch    16, train loss 1258.310701, train accuracy 23.78, val loss 260.751508, val error 77.69\n",
            "Epoch    17, train loss 1238.373320, train accuracy 24.64, val loss 257.524462, val error 77.22\n",
            "Epoch    18, train loss 1221.953628, train accuracy 25.44, val loss 254.179750, val error 76.38\n",
            "Epoch    19, train loss 1221.052246, train accuracy 26.65, val loss 253.634209, val error 75.34\n",
            "Epoch    20, train loss 1194.918005, train accuracy 27.47, val loss 249.240150, val error 74.67\n",
            "Epoch    21, train loss 1190.357547, train accuracy 27.77, val loss 248.784595, val error 75.11\n",
            "Epoch    22, train loss 1173.734442, train accuracy 28.56, val loss 245.761369, val error 73.75\n",
            "Epoch    23, train loss 1157.476364, train accuracy 29.28, val loss 242.968023, val error 73.10\n",
            "Epoch    24, train loss 1150.707654, train accuracy 29.76, val loss 241.897097, val error 73.05\n",
            "Epoch    25, train loss 1130.321799, train accuracy 30.23, val loss 238.412656, val error 72.79\n",
            "Epoch    26, train loss 1120.988694, train accuracy 31.59, val loss 236.716343, val error 71.56\n",
            "Epoch    27, train loss 1108.353112, train accuracy 32.12, val loss 234.785157, val error 71.94\n",
            "Epoch    28, train loss 1098.475973, train accuracy 32.29, val loss 233.133252, val error 71.52\n",
            "Epoch    29, train loss 1093.806805, train accuracy 32.64, val loss 232.272262, val error 71.54\n",
            "Epoch    30, train loss 1091.144184, train accuracy 33.31, val loss 231.814217, val error 70.40\n",
            "Epoch    31, train loss 1074.728650, train accuracy 33.65, val loss 229.292914, val error 70.41\n",
            "Epoch    32, train loss 1063.960809, train accuracy 34.79, val loss 227.810444, val error 69.95\n",
            "Epoch    33, train loss 1055.360340, train accuracy 34.45, val loss 226.199433, val error 69.62\n",
            "Epoch    34, train loss 1054.352684, train accuracy 34.87, val loss 226.131706, val error 69.54\n",
            "Epoch    35, train loss 1050.884834, train accuracy 34.94, val loss 225.934991, val error 69.21\n",
            "Epoch    36, train loss 1042.314641, train accuracy 36.22, val loss 224.038004, val error 68.50\n",
            "Epoch    37, train loss 1018.949536, train accuracy 36.29, val loss 220.696422, val error 68.43\n",
            "Epoch    38, train loss 1030.069971, train accuracy 36.04, val loss 222.736055, val error 69.16\n",
            "Epoch    39, train loss 1023.328517, train accuracy 36.42, val loss 221.663280, val error 68.43\n",
            "Epoch    40, train loss 1014.083651, train accuracy 36.80, val loss 220.482497, val error 68.55\n",
            "Epoch    41, train loss 1007.146325, train accuracy 37.34, val loss 219.388533, val error 68.04\n",
            "Epoch    42, train loss 1001.013097, train accuracy 38.06, val loss 218.103909, val error 67.30\n",
            "Epoch    43, train loss 994.448555, train accuracy 38.25, val loss 217.390891, val error 66.75\n",
            "Epoch    44, train loss 983.760668, train accuracy 38.45, val loss 215.714100, val error 67.15\n",
            "Epoch    45, train loss 986.715177, train accuracy 38.71, val loss 216.680761, val error 67.51\n",
            "Epoch    46, train loss 980.226920, train accuracy 38.93, val loss 215.443850, val error 67.13\n",
            "Epoch    47, train loss 982.103575, train accuracy 38.59, val loss 216.075483, val error 67.14\n",
            "Epoch    48, train loss 981.662961, train accuracy 37.95, val loss 216.875202, val error 67.31\n",
            "Epoch    49, train loss 970.265619, train accuracy 39.23, val loss 214.441023, val error 66.78\n",
            "Batch Size: 128, Optimizer: SGD, Learning Rate: 0.01 -> Accuracy: 42.448, Loss: 1823.9029690027237\n",
            "Epoch     0, train loss 1797.730128, train accuracy 1.51, val loss 363.231190, val error 98.52\n",
            "Epoch     1, train loss 1795.008724, train accuracy 1.70, val loss 362.719050, val error 98.19\n",
            "Epoch     2, train loss 1791.347174, train accuracy 1.97, val loss 361.987242, val error 98.09\n",
            "Epoch     3, train loss 1787.185242, train accuracy 2.29, val loss 361.149673, val error 97.80\n",
            "Epoch     4, train loss 1782.033720, train accuracy 2.81, val loss 360.108786, val error 97.40\n",
            "Epoch     5, train loss 1776.830449, train accuracy 3.07, val loss 359.075543, val error 96.92\n",
            "Epoch     6, train loss 1772.085205, train accuracy 3.21, val loss 358.142782, val error 96.86\n",
            "Epoch     7, train loss 1766.830093, train accuracy 3.38, val loss 357.096126, val error 96.59\n",
            "Epoch     8, train loss 1762.093915, train accuracy 3.51, val loss 356.174732, val error 96.50\n",
            "Epoch     9, train loss 1756.449312, train accuracy 3.59, val loss 355.058603, val error 96.39\n",
            "Epoch    10, train loss 1751.429406, train accuracy 3.82, val loss 354.106441, val error 96.35\n",
            "Epoch    11, train loss 1746.528497, train accuracy 4.00, val loss 353.138372, val error 96.20\n",
            "Epoch    12, train loss 1741.298956, train accuracy 4.10, val loss 352.115135, val error 96.15\n",
            "Epoch    13, train loss 1735.261583, train accuracy 4.24, val loss 350.936173, val error 95.97\n",
            "Epoch    14, train loss 1730.088168, train accuracy 4.39, val loss 349.930853, val error 95.84\n",
            "Epoch    15, train loss 1723.790617, train accuracy 4.56, val loss 348.668287, val error 95.48\n",
            "Epoch    16, train loss 1718.683665, train accuracy 4.76, val loss 347.688361, val error 95.16\n",
            "Epoch    17, train loss 1711.912597, train accuracy 4.99, val loss 346.360290, val error 94.93\n",
            "Epoch    18, train loss 1704.854003, train accuracy 5.28, val loss 344.949019, val error 94.69\n",
            "Epoch    19, train loss 1698.868056, train accuracy 5.30, val loss 343.773936, val error 94.76\n",
            "Epoch    20, train loss 1692.828957, train accuracy 5.64, val loss 342.584193, val error 94.33\n",
            "Epoch    21, train loss 1686.839156, train accuracy 5.78, val loss 341.397363, val error 94.17\n",
            "Epoch    22, train loss 1680.609982, train accuracy 6.02, val loss 340.179644, val error 94.11\n",
            "Epoch    23, train loss 1673.732727, train accuracy 6.19, val loss 338.806429, val error 93.84\n",
            "Epoch    24, train loss 1669.539385, train accuracy 6.41, val loss 338.001284, val error 93.78\n",
            "Epoch    25, train loss 1662.971374, train accuracy 6.62, val loss 336.711843, val error 93.42\n",
            "Epoch    26, train loss 1657.817300, train accuracy 6.85, val loss 335.727855, val error 93.17\n",
            "Epoch    27, train loss 1649.676568, train accuracy 7.19, val loss 334.152312, val error 92.91\n",
            "Epoch    28, train loss 1645.270537, train accuracy 7.54, val loss 333.308216, val error 92.38\n",
            "Epoch    29, train loss 1639.921344, train accuracy 7.84, val loss 332.250405, val error 92.14\n",
            "Epoch    30, train loss 1633.060348, train accuracy 8.05, val loss 330.939400, val error 91.77\n",
            "Epoch    31, train loss 1628.528320, train accuracy 8.49, val loss 330.026426, val error 91.47\n",
            "Epoch    32, train loss 1621.359968, train accuracy 8.78, val loss 328.661013, val error 91.28\n",
            "Epoch    33, train loss 1614.350098, train accuracy 9.03, val loss 327.280310, val error 90.83\n",
            "Epoch    34, train loss 1609.587568, train accuracy 9.40, val loss 326.365877, val error 90.65\n",
            "Epoch    35, train loss 1603.367902, train accuracy 9.67, val loss 325.174295, val error 90.29\n",
            "Epoch    36, train loss 1596.244524, train accuracy 9.89, val loss 323.743766, val error 90.16\n",
            "Epoch    37, train loss 1590.116267, train accuracy 10.18, val loss 322.583051, val error 89.87\n",
            "Epoch    38, train loss 1583.405595, train accuracy 10.44, val loss 321.304856, val error 89.81\n",
            "Epoch    39, train loss 1578.060186, train accuracy 10.83, val loss 320.304030, val error 89.60\n",
            "Epoch    40, train loss 1573.334905, train accuracy 11.08, val loss 319.383348, val error 89.47\n",
            "Epoch    41, train loss 1566.328510, train accuracy 11.33, val loss 318.035762, val error 89.10\n",
            "Epoch    42, train loss 1561.367949, train accuracy 11.67, val loss 317.104098, val error 88.81\n",
            "Epoch    43, train loss 1553.946135, train accuracy 11.80, val loss 315.710886, val error 88.68\n",
            "Epoch    44, train loss 1550.352747, train accuracy 12.01, val loss 314.976437, val error 88.25\n",
            "Epoch    45, train loss 1545.770742, train accuracy 12.22, val loss 314.163005, val error 87.99\n",
            "Epoch    46, train loss 1537.934791, train accuracy 12.54, val loss 312.607993, val error 87.75\n",
            "Epoch    47, train loss 1531.977150, train accuracy 12.75, val loss 311.513978, val error 87.76\n",
            "Epoch    48, train loss 1528.729653, train accuracy 12.82, val loss 310.999505, val error 87.27\n",
            "Epoch    49, train loss 1521.669693, train accuracy 13.05, val loss 309.609599, val error 87.29\n",
            "Batch Size: 128, Optimizer: SGD, Learning Rate: 0.001 -> Accuracy: 42.448, Loss: 1823.9029690027237\n",
            "Epoch     0, train loss 1801.146664, train accuracy 1.00, val loss 363.927970, val error 99.00\n",
            "Epoch     1, train loss 1801.027971, train accuracy 1.00, val loss 363.884377, val error 99.00\n",
            "Epoch     2, train loss 1801.101213, train accuracy 1.00, val loss 363.913367, val error 99.00\n",
            "Epoch     3, train loss 1801.056842, train accuracy 1.00, val loss 363.895250, val error 99.00\n",
            "Epoch     4, train loss 1801.049766, train accuracy 1.00, val loss 363.894217, val error 99.00\n",
            "Epoch     5, train loss 1801.137002, train accuracy 1.00, val loss 363.912806, val error 99.00\n",
            "Epoch     6, train loss 1801.022380, train accuracy 1.00, val loss 363.888033, val error 99.00\n",
            "Epoch     7, train loss 1801.039203, train accuracy 1.00, val loss 363.886581, val error 99.00\n",
            "Epoch     8, train loss 1801.070323, train accuracy 1.00, val loss 363.900164, val error 99.00\n",
            "Epoch     9, train loss 1801.089445, train accuracy 1.00, val loss 363.900520, val error 99.00\n",
            "Epoch    10, train loss 1801.005251, train accuracy 1.00, val loss 363.887740, val error 99.00\n",
            "Epoch    11, train loss 1801.052111, train accuracy 1.00, val loss 363.880672, val error 99.00\n",
            "Epoch    12, train loss 1801.104997, train accuracy 1.00, val loss 363.926041, val error 99.00\n",
            "Epoch    13, train loss 1801.220634, train accuracy 1.00, val loss 363.916047, val error 99.00\n",
            "Epoch    14, train loss 1801.117821, train accuracy 1.00, val loss 363.893665, val error 99.00\n",
            "Epoch    15, train loss 1801.164336, train accuracy 1.00, val loss 363.905344, val error 99.00\n",
            "Epoch    16, train loss 1800.954603, train accuracy 1.00, val loss 363.852591, val error 99.00\n",
            "Epoch    17, train loss 1801.167511, train accuracy 1.00, val loss 363.917248, val error 99.00\n",
            "Epoch    18, train loss 1801.065376, train accuracy 1.00, val loss 363.902171, val error 99.00\n",
            "Epoch    19, train loss 1801.126960, train accuracy 1.00, val loss 363.900420, val error 99.00\n",
            "Epoch    20, train loss 1801.116815, train accuracy 1.00, val loss 363.899067, val error 99.00\n",
            "Epoch    21, train loss 1801.041404, train accuracy 1.00, val loss 363.876035, val error 99.00\n",
            "Epoch    22, train loss 1801.198130, train accuracy 1.00, val loss 363.914962, val error 99.00\n",
            "Epoch    23, train loss 1801.059722, train accuracy 1.00, val loss 363.896561, val error 99.00\n",
            "Epoch    24, train loss 1801.039365, train accuracy 1.00, val loss 363.895589, val error 99.00\n",
            "Epoch    25, train loss 1801.191058, train accuracy 1.00, val loss 363.938287, val error 99.00\n",
            "Epoch    26, train loss 1801.057098, train accuracy 1.00, val loss 363.898695, val error 99.00\n",
            "Epoch    27, train loss 1801.134431, train accuracy 1.00, val loss 363.912432, val error 99.00\n",
            "Epoch    28, train loss 1801.114467, train accuracy 1.00, val loss 363.919421, val error 99.00\n",
            "Epoch    29, train loss 1801.148232, train accuracy 1.00, val loss 363.910832, val error 99.00\n",
            "Epoch    30, train loss 1801.188915, train accuracy 1.00, val loss 363.946286, val error 99.00\n",
            "Epoch    31, train loss 1801.034093, train accuracy 1.00, val loss 363.897492, val error 99.00\n",
            "Epoch    32, train loss 1801.018086, train accuracy 1.00, val loss 363.891098, val error 99.00\n",
            "Epoch    33, train loss 1801.085275, train accuracy 1.00, val loss 363.895312, val error 99.00\n",
            "Epoch    34, train loss 1800.997881, train accuracy 1.00, val loss 363.877872, val error 99.00\n",
            "Epoch    35, train loss 1801.119640, train accuracy 1.00, val loss 363.928385, val error 99.00\n",
            "Epoch    36, train loss 1801.213067, train accuracy 1.00, val loss 363.922894, val error 99.00\n",
            "Epoch    37, train loss 1801.100820, train accuracy 1.00, val loss 363.883821, val error 99.00\n",
            "Epoch    38, train loss 1801.106245, train accuracy 1.00, val loss 363.916995, val error 99.00\n",
            "Epoch    39, train loss 1801.221358, train accuracy 1.00, val loss 363.929602, val error 99.00\n",
            "Epoch    40, train loss 1801.131552, train accuracy 1.00, val loss 363.894763, val error 99.00\n",
            "Epoch    41, train loss 1801.079198, train accuracy 1.00, val loss 363.907086, val error 99.00\n",
            "Epoch    42, train loss 1801.088964, train accuracy 1.00, val loss 363.886656, val error 99.00\n",
            "Epoch    43, train loss 1801.010139, train accuracy 1.00, val loss 363.875789, val error 99.00\n",
            "Epoch    44, train loss 1801.054994, train accuracy 1.00, val loss 363.894177, val error 99.00\n",
            "Epoch    45, train loss 1801.097604, train accuracy 1.00, val loss 363.906338, val error 99.00\n",
            "Epoch    46, train loss 1801.117558, train accuracy 1.00, val loss 363.930505, val error 99.00\n",
            "Epoch    47, train loss 1801.119946, train accuracy 1.00, val loss 363.896954, val error 99.00\n",
            "Epoch    48, train loss 1801.004557, train accuracy 1.00, val loss 363.871300, val error 99.00\n",
            "Epoch    49, train loss 1801.135636, train accuracy 1.00, val loss 363.913791, val error 99.00\n",
            "Batch Size: 128, Optimizer: Adam, Learning Rate: 0.01 -> Accuracy: 42.448, Loss: 1823.9029690027237\n",
            "Epoch     0, train loss 1537.996060, train accuracy 10.90, val loss 312.130872, val error 89.59\n",
            "Epoch     1, train loss 1433.978614, train accuracy 14.96, val loss 292.874781, val error 85.99\n",
            "Epoch     2, train loss 1360.514584, train accuracy 18.50, val loss 279.319896, val error 82.75\n",
            "Epoch     3, train loss 1313.966064, train accuracy 21.42, val loss 270.965821, val error 79.79\n",
            "Epoch     4, train loss 1272.692251, train accuracy 22.06, val loss 263.823118, val error 79.70\n",
            "Epoch     5, train loss 1259.136035, train accuracy 23.92, val loss 261.309808, val error 78.33\n",
            "Epoch     6, train loss 1224.284393, train accuracy 26.30, val loss 255.154500, val error 76.11\n",
            "Epoch     7, train loss 1190.365948, train accuracy 27.58, val loss 249.331063, val error 74.97\n",
            "Epoch     8, train loss 1171.552494, train accuracy 28.70, val loss 246.292939, val error 74.32\n",
            "Epoch     9, train loss 1152.093114, train accuracy 29.17, val loss 242.979587, val error 73.96\n",
            "Epoch    10, train loss 1141.027179, train accuracy 29.34, val loss 241.696092, val error 73.66\n",
            "Epoch    11, train loss 1125.171671, train accuracy 30.59, val loss 238.555322, val error 73.25\n",
            "Epoch    12, train loss 1106.731544, train accuracy 31.42, val loss 235.694315, val error 72.02\n",
            "Epoch    13, train loss 1092.870359, train accuracy 32.20, val loss 233.412308, val error 71.92\n",
            "Epoch    14, train loss 1075.715853, train accuracy 32.84, val loss 230.726648, val error 71.57\n",
            "Epoch    15, train loss 1078.946014, train accuracy 32.98, val loss 231.445376, val error 71.53\n",
            "Epoch    16, train loss 1066.768816, train accuracy 33.56, val loss 229.714674, val error 71.15\n",
            "Epoch    17, train loss 1059.460215, train accuracy 33.61, val loss 228.675085, val error 70.08\n",
            "Epoch    18, train loss 1049.984459, train accuracy 34.53, val loss 227.581747, val error 70.28\n",
            "Epoch    19, train loss 1042.657188, train accuracy 34.75, val loss 225.817682, val error 70.11\n",
            "Epoch    20, train loss 1051.606839, train accuracy 34.01, val loss 227.868727, val error 70.56\n",
            "Epoch    21, train loss 1037.678137, train accuracy 35.75, val loss 225.531177, val error 69.69\n",
            "Epoch    22, train loss 1006.172539, train accuracy 36.59, val loss 221.028738, val error 68.98\n",
            "Epoch    23, train loss 1024.490101, train accuracy 36.01, val loss 224.012589, val error 69.94\n",
            "Epoch    24, train loss 1007.787066, train accuracy 36.23, val loss 221.616687, val error 69.06\n",
            "Epoch    25, train loss 1002.070758, train accuracy 36.60, val loss 220.657708, val error 68.94\n",
            "Epoch    26, train loss 994.809286, train accuracy 37.55, val loss 220.010129, val error 68.80\n",
            "Epoch    27, train loss 990.638943, train accuracy 37.46, val loss 220.292168, val error 68.57\n",
            "Epoch    28, train loss 975.890413, train accuracy 38.11, val loss 217.552859, val error 68.25\n",
            "Epoch    29, train loss 997.075577, train accuracy 36.96, val loss 221.320714, val error 68.81\n",
            "Epoch    30, train loss 973.188332, train accuracy 38.17, val loss 217.708998, val error 68.26\n",
            "Epoch    31, train loss 965.916081, train accuracy 38.87, val loss 216.627246, val error 67.79\n",
            "Epoch    32, train loss 973.111403, train accuracy 38.66, val loss 218.201999, val error 67.77\n",
            "Epoch    33, train loss 966.188550, train accuracy 38.38, val loss 217.585881, val error 67.93\n",
            "Epoch    34, train loss 957.847501, train accuracy 39.22, val loss 216.099711, val error 67.76\n",
            "Epoch    35, train loss 963.198462, train accuracy 39.06, val loss 217.983159, val error 68.08\n",
            "Epoch    36, train loss 946.354141, train accuracy 39.87, val loss 215.573422, val error 67.83\n",
            "Epoch    37, train loss 946.238166, train accuracy 39.66, val loss 214.828001, val error 67.44\n",
            "Epoch    38, train loss 966.748483, train accuracy 38.57, val loss 219.431004, val error 68.49\n",
            "Epoch    39, train loss 945.737672, train accuracy 40.04, val loss 215.200170, val error 67.40\n",
            "Epoch    40, train loss 934.294601, train accuracy 40.19, val loss 213.719688, val error 67.17\n",
            "Epoch    41, train loss 932.056294, train accuracy 40.69, val loss 213.627002, val error 66.86\n",
            "Epoch    42, train loss 928.165069, train accuracy 40.82, val loss 213.939424, val error 67.38\n",
            "Epoch    43, train loss 930.832710, train accuracy 40.99, val loss 213.660953, val error 67.31\n",
            "Epoch    44, train loss 936.621974, train accuracy 40.57, val loss 215.545505, val error 67.74\n",
            "Epoch    45, train loss 915.207387, train accuracy 41.56, val loss 211.734466, val error 66.81\n",
            "Epoch    46, train loss 927.596896, train accuracy 41.00, val loss 214.071585, val error 67.31\n",
            "Epoch    47, train loss 920.274513, train accuracy 40.92, val loss 213.124735, val error 67.35\n",
            "Epoch    48, train loss 916.820161, train accuracy 41.32, val loss 212.883057, val error 67.14\n",
            "Epoch    49, train loss 915.381863, train accuracy 41.53, val loss 212.736875, val error 66.74\n",
            "Batch Size: 128, Optimizer: Adam, Learning Rate: 0.001 -> Accuracy: 42.448, Loss: 1823.9029690027237\n",
            "Best Parameters: {'batch_size': 64, 'optimizer': 'SGD', 'learning_rate': 0.01}\n",
            "Best Accuracy: 42.448\n",
            "Best Train Loss: 1823.9029690027237\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters Search\n",
        "\n",
        "best_accuracy = 0\n",
        "best_train_loss = float('inf')\n",
        "best_model = None\n",
        "best_params = {}\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "for i in batch_sizes:\n",
        "  for optimizer_name in optimizers:\n",
        "    for k in learning_rate:\n",
        "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=i, shuffle=True, num_workers=num_workers)\n",
        "      testloader = torch.utils.data.DataLoader(testset, batch_size=i, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "      n_epoch = 50\n",
        "      model = ConvNetDA520()\n",
        "      model.apply(weights_init)  # Initialize model weights in each hyperparameter set\n",
        "\n",
        "      optimizer = get_optimizer(model, optimizer_name, k)\n",
        "      trained_model2, errors_train2, errors_val2, losses_train2, losses_val2, train_accuracy2, val_accuracy2 = train(n_epoch, model)\n",
        "\n",
        "      final_error = errors_train2[-1]\n",
        "      final_accuracy = train_accuracy2[-1]\n",
        "      final_loss = losses_train2[-1]\n",
        "\n",
        "      if final_accuracy > best_accuracy:\n",
        "        best_accuracy = final_accuracy\n",
        "        best_train_loss = final_loss\n",
        "        best_model = trained_model2\n",
        "        best_params = {'batch_size': i, 'optimizer': optimizer_name, 'learning_rate': k}\n",
        "\n",
        "      print(f\"Batch Size: {i}, Optimizer: {optimizer_name}, Learning Rate: {k} -> Accuracy: {best_accuracy}, Loss: {best_train_loss}\")\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "print(\"Best Train Loss:\", best_train_loss)\n",
        "\n",
        "# save best performing model to be deployed together with the hyperparameters\n",
        "PATH = f\"best_model_hyperparam_cifar100_{groupid}\"\n",
        "torch.save(best_model, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    'activation_fn': activation_fn,\n",
        "    'learning_rate': 0.01,\n",
        "    'n_epoch': 50,\n",
        "    'optimizer': 'SGD',\n",
        "    'batch_size': 64\n",
        "}\n",
        "import pickle\n",
        "PATH_HYP = f\"hyperparameters_cifar100_{groupid}\"\n",
        "with open(PATH_HYP, 'wb') as f:\n",
        "    pickle.dump(hyperparameters, f)"
      ],
      "metadata": {
        "id": "xDZBBvSSadZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02E_DAR4sJhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b56410-4078-49e9-8cfb-f87f84ab608d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.0211, Accuracy: 3368/10000 (34%)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.021108711981773377, tensor(33.6800))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "test(best_model)\n",
        "# Best hyperparameter set is batch size: 64 - optimizer: SGD - learning_rate: 0.01\n",
        "# Due to performance issues, we were not able to try more combinations.\n",
        "# The best model with this hyperparameter set has a train accuracy of 42.48\n",
        "# The best model with this hyperparameter set has a test accuracy of 34% as seen below"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy # The best model with this hyperparameter set has a train accuracy of 42.48"
      ],
      "metadata": {
        "id": "3ag3_FkUrOp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6955fc-085f-4938-9ad5-699c6ed02b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42.448"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**While developing new architecture for convolutional neural network, we faced a problem where accuracy stayed the same in all epochs despite setting every step correct. So, we continued this part in another Colab file named \"ClassNewArch_Second_Approach_DA520\"**"
      ],
      "metadata": {
        "id": "pR12CfObYQbU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8noZ-NF05Lo"
      },
      "source": [
        "##### Save the Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji_seiu70ipd"
      },
      "outputs": [],
      "source": [
        "# We saved our final model, which is found in hyperparameter search, above. It is named \"best_model_hyperparam_cifar100\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}