{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "print(\"Using torch\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0wbU4JQvI_k",
        "outputId": "d128b31e-074b-4b3e-f755-f837d4d9a00e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.3.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeyiJ6Jru00F",
        "outputId": "3ae6e589-801d-4b4f-fdd3-d2823a7a11a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7dfa041ec2b0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Setting the seed\n",
        "# replace student id (here 34xxx) with last 5 digits of your own student id\n",
        "studentid = 34772\n",
        "torch.manual_seed(studentid)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "breastcancer = load_breast_cancer()\n",
        "print(breastcancer.data.shape, breastcancer.target.shape)\n",
        "print(breastcancer.feature_names)\n",
        "print(breastcancer.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn7xlw1cwP87",
        "outputId": "49bc55cd-f54a-4fff-d2a6-6df606f5d163"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30) (569,)\n",
            "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
            "['malignant' 'benign']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = breastcancer.data\n",
        "y = breastcancer.target\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEKrA_ZZ3loe",
        "outputId": "0247c4d8-42df-4a84-c031-3474c3a7a81c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 30) (569,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divide dataset into TRAIN, VALIDATION and TEST\n",
        "from sklearn.model_selection import train_test_split\n",
        "test_size = 0.1\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, stratify=y, random_state=studentid)\n",
        "print(X_train_val.shape, y_train_val.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDj05X9m3xOU",
        "outputId": "16fd5c4c-ba7b-43a9-b3c1-1179698a2055"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(512, 30) (512,) (57, 30) (57,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_val = torch.tensor(X_train_val, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_val = torch.tensor(y_train_val, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "y_train_val = y_train_val.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "print(X_train_val.shape, X_test.shape, y_train_val.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9Czw1ya8_bS",
        "outputId": "cd302a09-bb9d-474b-9369-5b614c9a17b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 30]) torch.Size([57, 30]) torch.Size([512, 1]) torch.Size([57, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# performs 5-fold cross validation to evaluate your models with 20% validation set\n",
        "# hyperparameter selection should be done using X_train_val, y_train_val dataset\n",
        "val_size = 0.2"
      ],
      "metadata": {
        "id": "cUN2v9Xd4f4k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set model hyperparameters\n",
        "d_in = X_train_val.shape[1] # this is fixed\n",
        "d_out = 1 #this is also fixed because it is a binary classification problem\n",
        "\n",
        "# use three different hidden layer configuration, one of which must be a network with 3-hidden layers (80, 40, 20)\n",
        "h_layers = [(80, 40, 20), (60,30,15,10), (100,50,25,10,5)]\n",
        "\n",
        "# use Sigmoid, Tanh, ReLU and LeakyReLU activation functions\n",
        "# -> use the same activation function across the neural network\n",
        "activation_fn = [nn.Sigmoid(), nn.ReLU(), nn.Tanh(), nn.LeakyReLU() ]\n",
        "\n",
        "learning_rate = [0.01, 0.001, 0.0001]\n",
        "n_epochs = [200,250,300]\n",
        "d_out = 1"
      ],
      "metadata": {
        "id": "o_OebqiimHem"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating A Function for Model"
      ],
      "metadata": {
        "id": "EAUYA6MdFEOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(d_in, h_layers, activation_fn, d_out):\n",
        "  layers = []\n",
        "\n",
        "  # Input layer\n",
        "  layers.append(nn.Linear(d_in, h_layers[0], bias=True))\n",
        "  layers.append(activation_fn)\n",
        "\n",
        "  # Hidden layers\n",
        "  for i in range(len(h_layers) - 1):\n",
        "    layers.append(nn.Linear(h_layers[i], h_layers[i+1]))\n",
        "    layers.append(activation_fn)\n",
        "\n",
        "  # Output layer\n",
        "  layers.append(nn.Linear(h_layers[-1], d_out))\n",
        "  layers.append(nn.Sigmoid()) #output transformation - due to being a binary classification problem we need Sigmoid function here\n",
        "\n",
        "  model = nn.Sequential(*layers)\n",
        "  return model"
      ],
      "metadata": {
        "id": "QLgxdKEyZT1R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_model(d_in, h_layers[0], nn.Sigmoid(), d_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lhy2uHLJTcA",
        "outputId": "0e162efd-c3f0-4613-a9de-d79c3d20a11b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=30, out_features=80, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=80, out_features=40, bias=True)\n",
              "  (3): Sigmoid()\n",
              "  (4): Linear(in_features=40, out_features=20, bias=True)\n",
              "  (5): Sigmoid()\n",
              "  (6): Linear(in_features=20, out_features=1, bias=True)\n",
              "  (7): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Loss and Optimizer"
      ],
      "metadata": {
        "id": "5DAz-Xp7FKd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function and optimizer\n",
        "# use binary cross entropy loss and Adam optimizer\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# use 3 different learning rates 0.01, 0.001 and 0.0001\n",
        "learning_rate = [0.01, 0.001, 0.0001]\n",
        "\n",
        "def create_optimizer(model, learning_rate):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  return optimizer"
      ],
      "metadata": {
        "id": "2-BXsl4b1mzN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Function for Model Training"
      ],
      "metadata": {
        "id": "OvrAsEtQFRWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, learning_rate, epochs, optimizer):\n",
        "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    # forward pass\n",
        "    y_pred = model(X_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    # backward pass\n",
        "    # zero out grads\n",
        "    optimizer.zero_grad()\n",
        "    # compute gradients\n",
        "    loss.backward()\n",
        "    # update\n",
        "    optimizer.step()\n",
        "  #model.eval()\n",
        "  #y_pred = model(X_test)\n",
        "  #loss = loss_fn(y_pred, y_test)\n",
        "  #print(f\"Epoch {epoch} loss: {float(loss)}\")\n",
        "  #accuracy = accuracy_score(y_test, y_pred)\n",
        "  #f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Gd4nJm7NZgE5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validate_model(d_in, h_layers, activation_fn, d_out, learning_rate, X_train, y_train, epochs, n_splits=5):\n",
        "  kf = KFold(n_splits=n_splits, shuffle=True, random_state = studentid)\n",
        "  accuracies = []\n",
        "  f1_scores = []\n",
        "  losses = []\n",
        "\n",
        "  for train_index, val_index in kf.split(X_train):\n",
        "    X_train_cv, X_test_cv = X_train[train_index], X_train[val_index]\n",
        "    y_train_cv, y_test_cv = y_train[train_index], y_train[val_index]\n",
        "\n",
        "    model = create_model(d_in, h_layers, activation_fn, d_out)\n",
        "    optimizer = create_optimizer(model, learning_rate)\n",
        "\n",
        "    model = train_model(model, X_train_cv, y_train_cv, learning_rate, epochs, optimizer)\n",
        "\n",
        "    model.eval()\n",
        "    y_pred = model(X_test_cv)\n",
        "    loss = loss_fn(y_pred, y_test_cv)\n",
        "\n",
        "    y_pred_np = y_pred.detach().numpy()\n",
        "    y_pred_binary = (y_pred_np >= 0.5).astype(int)\n",
        "\n",
        "    accuracies.append(accuracy_score(y_test_cv, y_pred_binary))\n",
        "    f1_scores.append(f1_score(y_test_cv, y_pred_binary))\n",
        "    losses.append(loss.detach().numpy())\n",
        "    #print(f\"Epoch {epoch} loss: {float(loss)}\")\n",
        "\n",
        "  return np.mean(accuracies), np.mean(f1_scores), np.mean(losses)"
      ],
      "metadata": {
        "id": "Cs2gPkkReH4u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_accuracy, mean_f1, losses = cross_validate_model(d_in, h_layers[0], activation_fn[0], d_out, 0.01, X_train_val, y_train_val, epochs=200)"
      ],
      "metadata": {
        "id": "tKi-1r1WmS7R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iay58Wo5Doa_",
        "outputId": "00fe9b70-d635-4cbf-b91b-665475548470"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20041709"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# performs 5-fold cross validation (use mean of the val-scores) and select best performing model\n",
        "# Plot the training and validation accuracy/loss curves.\n",
        "\n",
        "best_accuracy = 0\n",
        "best_f1 = 0\n",
        "best_params = {}\n",
        "losses = []\n",
        "epochs = []\n",
        "\n",
        "for i in activation_fn:\n",
        "  for j in h_layers:\n",
        "    for k in learning_rate:\n",
        "      for t in n_epochs:\n",
        "        mean_accuracy, mean_f, mean_loss = cross_validate_model(d_in, j, i, d_out, k, X_train_val, y_train_val, epochs=t)\n",
        "        print(f\"Params: {i}, {j}, {k} {t} -> Accuracy: {mean_accuracy}, F1 Score: {mean_f1}\")\n",
        "        losses.append(mean_loss)\n",
        "        epochs.append(t)\n",
        "\n",
        "\n",
        "        if mean_accuracy > best_accuracy:\n",
        "          best_accuracy = mean_accuracy\n",
        "          best_f1 = mean_f1\n",
        "          best_params = {'hidden_layers': j, 'activation': i, 'learning_rate': k, 'epochs': t}\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "print(\"Best F1 Score:\", best_f1)"
      ],
      "metadata": {
        "id": "_zvtNkBJ50L-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193515cf-8b64-4bcd-b3a6-e9ba48c4f5b0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: Sigmoid(), (80, 40, 20), 0.01 200 -> Accuracy: 0.8767942128307634, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (80, 40, 20), 0.01 250 -> Accuracy: 0.9238339996192652, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (80, 40, 20), 0.01 300 -> Accuracy: 0.9413858747382449, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (80, 40, 20), 0.001 200 -> Accuracy: 0.9141062250142775, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (80, 40, 20), 0.001 250 -> Accuracy: 0.9355606320197982, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (80, 40, 20), 0.001 300 -> Accuracy: 0.9199124309918142, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (80, 40, 20), 0.0001 200 -> Accuracy: 0.626956025128498, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (80, 40, 20), 0.0001 250 -> Accuracy: 0.626956025128498, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (80, 40, 20), 0.0001 300 -> Accuracy: 0.626956025128498, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (60, 30, 15, 10), 0.01 200 -> Accuracy: 0.919931467732724, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (60, 30, 15, 10), 0.01 250 -> Accuracy: 0.9199124309918142, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (60, 30, 15, 10), 0.01 300 -> Accuracy: 0.9179706834189986, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (60, 30, 15, 10), 0.001 200 -> Accuracy: 0.8553017323434228, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (60, 30, 15, 10), 0.001 250 -> Accuracy: 0.7420711974110034, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (60, 30, 15, 10), 0.001 300 -> Accuracy: 0.9160860460689131, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (60, 30, 15, 10), 0.0001 200 -> Accuracy: 0.5157814582143537, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (60, 30, 15, 10), 0.0001 250 -> Accuracy: 0.5661907481439178, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (60, 30, 15, 10), 0.0001 300 -> Accuracy: 0.626956025128498, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (100, 50, 25, 10, 5), 0.01 200 -> Accuracy: 0.8003045878545592, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (100, 50, 25, 10, 5), 0.01 250 -> Accuracy: 0.9180087569008185, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (100, 50, 25, 10, 5), 0.01 300 -> Accuracy: 0.8378450409289929, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (100, 50, 25, 10, 5), 0.001 200 -> Accuracy: 0.6754997144488863, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (100, 50, 25, 10, 5), 0.001 250 -> Accuracy: 0.8454978107747954, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (100, 50, 25, 10, 5), 0.001 300 -> Accuracy: 0.8499904816295449, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (100, 50, 25, 10, 5), 0.0001 200 -> Accuracy: 0.5667618503712164, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (100, 50, 25, 10, 5), 0.0001 250 -> Accuracy: 0.626956025128498, F1 Score: 0.937929605907253\n",
            "Params: Sigmoid(), (100, 50, 25, 10, 5), 0.0001 300 -> Accuracy: 0.5273557966876071, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (80, 40, 20), 0.01 200 -> Accuracy: 0.9375594898153435, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (80, 40, 20), 0.01 250 -> Accuracy: 0.9335617742242528, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (80, 40, 20), 0.01 300 -> Accuracy: 0.8812488102036932, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (80, 40, 20), 0.001 200 -> Accuracy: 0.9277936417285362, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (80, 40, 20), 0.001 250 -> Accuracy: 0.9296973158195317, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (80, 40, 20), 0.001 300 -> Accuracy: 0.9336188844469827, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (80, 40, 20), 0.0001 200 -> Accuracy: 0.8984199505044737, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (80, 40, 20), 0.0001 250 -> Accuracy: 0.9042832667047401, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (80, 40, 20), 0.0001 300 -> Accuracy: 0.912145440700552, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (60, 30, 15, 10), 0.01 200 -> Accuracy: 0.9375404530744337, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (60, 30, 15, 10), 0.01 250 -> Accuracy: 0.9375214163335238, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (60, 30, 15, 10), 0.01 300 -> Accuracy: 0.9453074433656958, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (60, 30, 15, 10), 0.001 200 -> Accuracy: 0.9219303255282696, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (60, 30, 15, 10), 0.001 250 -> Accuracy: 0.9297734627831715, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (60, 30, 15, 10), 0.001 300 -> Accuracy: 0.9180087569008185, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (60, 30, 15, 10), 0.0001 200 -> Accuracy: 0.7945174186179326, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (60, 30, 15, 10), 0.0001 250 -> Accuracy: 0.9180658671235484, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (60, 30, 15, 10), 0.0001 300 -> Accuracy: 0.8043023034456501, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (100, 50, 25, 10, 5), 0.01 200 -> Accuracy: 0.800894726822768, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (100, 50, 25, 10, 5), 0.01 250 -> Accuracy: 0.8753854940034266, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (100, 50, 25, 10, 5), 0.01 300 -> Accuracy: 0.7531696173615077, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (100, 50, 25, 10, 5), 0.001 200 -> Accuracy: 0.9355796687607082, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (100, 50, 25, 10, 5), 0.001 250 -> Accuracy: 0.9296973158195317, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (100, 50, 25, 10, 5), 0.001 300 -> Accuracy: 0.8792689891490578, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (100, 50, 25, 10, 5), 0.0001 200 -> Accuracy: 0.7499143346659052, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (100, 50, 25, 10, 5), 0.0001 250 -> Accuracy: 0.7451932229202359, F1 Score: 0.937929605907253\n",
            "Params: ReLU(), (100, 50, 25, 10, 5), 0.0001 300 -> Accuracy: 0.8711022272986865, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (80, 40, 20), 0.01 200 -> Accuracy: 0.9120883304778221, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (80, 40, 20), 0.01 250 -> Accuracy: 0.9220064724919095, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (80, 40, 20), 0.01 300 -> Accuracy: 0.8926898914905769, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (80, 40, 20), 0.001 200 -> Accuracy: 0.9179326099371787, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (80, 40, 20), 0.001 250 -> Accuracy: 0.9198743575099944, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (80, 40, 20), 0.001 300 -> Accuracy: 0.9375785265562536, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (80, 40, 20), 0.0001 200 -> Accuracy: 0.9023605558728345, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (80, 40, 20), 0.0001 250 -> Accuracy: 0.8965543498952979, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (80, 40, 20), 0.0001 300 -> Accuracy: 0.896516276413478, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (60, 30, 15, 10), 0.01 200 -> Accuracy: 0.8967066438225775, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (60, 30, 15, 10), 0.01 250 -> Accuracy: 0.9024747763182942, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (60, 30, 15, 10), 0.01 300 -> Accuracy: 0.8688939653531316, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (60, 30, 15, 10), 0.001 200 -> Accuracy: 0.9140300780506377, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (60, 30, 15, 10), 0.001 250 -> Accuracy: 0.9238149628783552, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (60, 30, 15, 10), 0.001 300 -> Accuracy: 0.9257947839329906, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (60, 30, 15, 10), 0.0001 200 -> Accuracy: 0.8475537787930707, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (60, 30, 15, 10), 0.0001 250 -> Accuracy: 0.8847325337902152, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (60, 30, 15, 10), 0.0001 300 -> Accuracy: 0.8946506758043024, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (100, 50, 25, 10, 5), 0.01 200 -> Accuracy: 0.7524462212069294, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (100, 50, 25, 10, 5), 0.01 250 -> Accuracy: 0.7401484865790977, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (100, 50, 25, 10, 5), 0.01 300 -> Accuracy: 0.626956025128498, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (100, 50, 25, 10, 5), 0.001 200 -> Accuracy: 0.9238720731010851, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (100, 50, 25, 10, 5), 0.001 250 -> Accuracy: 0.9140681515324577, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (100, 50, 25, 10, 5), 0.001 300 -> Accuracy: 0.9140491147915476, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (100, 50, 25, 10, 5), 0.0001 200 -> Accuracy: 0.9003807348181991, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (100, 50, 25, 10, 5), 0.0001 250 -> Accuracy: 0.9043594136683799, F1 Score: 0.937929605907253\n",
            "Params: Tanh(), (100, 50, 25, 10, 5), 0.0001 300 -> Accuracy: 0.900399771559109, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (80, 40, 20), 0.01 200 -> Accuracy: 0.8851323053493243, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (80, 40, 20), 0.01 250 -> Accuracy: 0.874814391776128, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (80, 40, 20), 0.01 300 -> Accuracy: 0.8145250333142966, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (80, 40, 20), 0.001 200 -> Accuracy: 0.9336379211878928, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (80, 40, 20), 0.001 250 -> Accuracy: 0.9356177422425281, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (80, 40, 20), 0.001 300 -> Accuracy: 0.9336379211878926, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (80, 40, 20), 0.0001 200 -> Accuracy: 0.892594707786027, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (80, 40, 20), 0.0001 250 -> Accuracy: 0.9160098991052731, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (80, 40, 20), 0.0001 300 -> Accuracy: 0.9043594136683799, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (60, 30, 15, 10), 0.01 200 -> Accuracy: 0.9414239482200648, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (60, 30, 15, 10), 0.01 250 -> Accuracy: 0.9394631639063393, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (60, 30, 15, 10), 0.01 300 -> Accuracy: 0.9433656957928802, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (60, 30, 15, 10), 0.001 200 -> Accuracy: 0.9394250904245194, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (60, 30, 15, 10), 0.001 250 -> Accuracy: 0.9336379211878928, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (60, 30, 15, 10), 0.001 300 -> Accuracy: 0.927812678469446, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (60, 30, 15, 10), 0.0001 200 -> Accuracy: 0.8945554920997525, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (60, 30, 15, 10), 0.0001 250 -> Accuracy: 0.7936607652769846, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (60, 30, 15, 10), 0.0001 300 -> Accuracy: 0.9140871882733675, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (100, 50, 25, 10, 5), 0.01 200 -> Accuracy: 0.9433656957928804, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (100, 50, 25, 10, 5), 0.01 250 -> Accuracy: 0.9316771368741673, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (100, 50, 25, 10, 5), 0.01 300 -> Accuracy: 0.9511517228250523, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (100, 50, 25, 10, 5), 0.001 200 -> Accuracy: 0.9199505044736341, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (100, 50, 25, 10, 5), 0.001 250 -> Accuracy: 0.9355796687607082, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (100, 50, 25, 10, 5), 0.001 300 -> Accuracy: 0.9434037692747003, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (100, 50, 25, 10, 5), 0.0001 200 -> Accuracy: 0.7446601941747573, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (100, 50, 25, 10, 5), 0.0001 250 -> Accuracy: 0.9023605558728345, F1 Score: 0.937929605907253\n",
            "Params: LeakyReLU(negative_slope=0.01), (100, 50, 25, 10, 5), 0.0001 300 -> Accuracy: 0.9102036931277364, F1 Score: 0.937929605907253\n",
            "Best Parameters: {'hidden_layers': (100, 50, 25, 10, 5), 'activation': LeakyReLU(negative_slope=0.01), 'learning_rate': 0.01, 'epochs': 300}\n",
            "Best Accuracy: 0.9511517228250523\n",
            "Best F1 Score: 0.937929605907253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Best parameters turned out to be:\n",
        "Best Parameters: {'hidden_layers': (60, 30, 15, 10), 'activation': LeakyReLU(negative_slope=0.01), 'learning_rate': 0.01, 'epochs': 250}\n",
        "Best Accuracy: 0.9512278697886922\n",
        "Best F1 Score: 0.937929605907253\n",
        "Now I will train the model with the best parameters\n",
        "'''"
      ],
      "metadata": {
        "id": "n6CK3I1JLyIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# divide dataset into TRAIN, VALIDATION and TEST\n",
        "#for avoiding any mix-ups with above functions, I run the splits again here with the same random_state to obtain the initial X test and X trains\n",
        "from sklearn.model_selection import train_test_split\n",
        "test_size = 0.1\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, stratify=y, random_state=studentid)\n",
        "print(X_train_val.shape, y_train_val.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "X_train_val = torch.tensor(X_train_val, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_val = torch.tensor(y_train_val, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "y_train_val = y_train_val.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzB1uH6t1-sC",
        "outputId": "ae5f3060-24a2-4984-8067-f2e8b47b9c1f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(512, 30) (512,) (57, 30) (57,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train model with whole train_val dataset\n",
        "model2deploy = create_model(d_in, best_params['hidden_layers'], best_params['activation'], d_out)\n",
        "optimizer = optim.Adam(model2deploy.parameters(), best_params['learning_rate'])\n",
        "train_model(model2deploy, X_train_val, y_train_val, best_params['learning_rate'], best_params['epochs'], optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_MMwg1wzDe7",
        "outputId": "ebdb5005-2efb-4745-8976-0e22a1ddc9fb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=30, out_features=100, bias=True)\n",
              "  (1): LeakyReLU(negative_slope=0.01)\n",
              "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
              "  (3): LeakyReLU(negative_slope=0.01)\n",
              "  (4): Linear(in_features=50, out_features=25, bias=True)\n",
              "  (5): LeakyReLU(negative_slope=0.01)\n",
              "  (6): Linear(in_features=25, out_features=10, bias=True)\n",
              "  (7): LeakyReLU(negative_slope=0.01)\n",
              "  (8): Linear(in_features=10, out_features=5, bias=True)\n",
              "  (9): LeakyReLU(negative_slope=0.01)\n",
              "  (10): Linear(in_features=5, out_features=1, bias=True)\n",
              "  (11): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance of the best performing model on the test dataset\n",
        "\n",
        "model2deploy.eval()\n",
        "y_pred = model2deploy(X_test)\n",
        "test_loss = loss_fn(y_pred, y_test)\n",
        "\n",
        "y_pred_np = y_pred.detach().numpy() #converting to numpy array in order to perform operations\n",
        "y_pred_binary = (y_pred_np >= 0.5).astype(int)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "\n",
        "print(f\"Test accuracy: {test_accuracy}, Test loss: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy6nbfok0eeE",
        "outputId": "71554392-f6e8-4c0e-ec3d-1fe217cad956"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9649122807017544, Test loss: 0.0848802998661995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save best performing model to be deployed together with the hyperparameters\n",
        "PATH = f\"model_breast_cancer_{studentid}\"\n",
        "torch.save(model2deploy, PATH)\n",
        "# save hyperparameters values for the best performing model\n",
        "hyperparameters = {\n",
        "    'layer_sizes': best_params['hidden_layers'],\n",
        "    'activation_fn': best_params['activation'],\n",
        "    'learning_rate': best_params['learning_rate'],\n",
        "    'n_epoch': best_params['epochs']\n",
        "}\n",
        "import pickle\n",
        "PATH_HYP = f\"hyperparameters_{studentid}\"\n",
        "with open(PATH_HYP, 'wb') as f:\n",
        "    pickle.dump(hyperparameters, f)"
      ],
      "metadata": {
        "id": "cFGOs0Dg6fpC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I defined a function for creating the model. I needed that function because for each hyperparameter combination, my model would be different.\n",
        "\n",
        "Then, I defined my loss function. (binary cross entropy loss)\n",
        "\n",
        "After that, I created another function for Optimizer, because there are again, different learning rates\n",
        "\n",
        "I created a function for model training, I needed this for cross validation and hyperparameter tuning.\n",
        "\n",
        "Lastly, I defined a function for implementing cross validations\n",
        "\n",
        "My cross validation function included other functions in order to work.\n",
        "I created for loops, for hyperparameter combinations. Each time, I kept that combinations average accuracy and F1 score (cross validation folds' average scores)\n",
        "\n",
        "Using that average accuracy and F1 score lists, I obtained the best model and best parameters.\n",
        "\n",
        "By using the best parameters, I trained my model on the whole X_train_val dataset.\n",
        "\n",
        "Using this newly trained model2deploy named model, I made predictions for the test dataset and found the related accuracy and F1 scores."
      ],
      "metadata": {
        "id": "9Cbv9tqIIaqN"
      }
    }
  ]
}